{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "““坚果改造笔画cuda_error”的副本”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiHanWang2Developer/data/blob/master/outputs%5B%22instances%22%5D.pred_masks\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-UPaAdWoVgJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR",
        "colab_type": "text"
      },
      "source": [
        "# [How to train Detectron2 with Custom COCO Datasets](https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/) | DLology\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "This notebook will help you get started with this framwork by training a instance segmentation model with your custom COCO datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bVqmEoGK4jf",
        "colab_type": "text"
      },
      "source": [
        "本文参考https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDC4G20IuIm",
        "colab_type": "code",
        "outputId": "932bada5-1c51-4bd1-b96a-61da61c8739e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr  4 16:01:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    34W / 250W |   3573MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII",
        "colab_type": "text"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "1d20ce04-1a2d-4245-be7a-1f13f8d42f48"
      },
      "source": [
        "!pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "import torch, torchvision\n",
        "torch.__version__"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Collecting git+https://github.com/facebookresearch/fvcore.git\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-lb3el4xd\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-lb3el4xd\n",
            "Requirement already satisfied (use --upgrade to upgrade): fvcore==0.1 from git+https://github.com/facebookresearch/fvcore.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.18.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (0.1.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (5.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (4.38.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.6.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1) (0.8.7)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1-cp36-none-any.whl size=42662 sha256=420e55f28b337b0f5a29fd839d2569f780f25a749ae775ae185758879ecfb900\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iiv49ww2/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n",
            "Successfully built fvcore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeejixTmwEmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "b8bf9f3d-bcee-424a-bd95-e67f9a9b72f3"
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
        "# clone the repo to access PointRend code. Use the same version as the installed detectron2\n",
        "!git clone --branch v0.1.1 https://github.com/facebookresearch/detectron2 detectron2_repo"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
            "Requirement already satisfied: detectron2 in /usr/local/lib/python3.6/dist-packages (0.1.1+cu100)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.2.0)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2) (7.0.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.38.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.21.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.9.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (46.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.6.0.post2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.27.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2) (5.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.1.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n",
            "fatal: destination path 'detectron2_repo' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# import PointRend project\n",
        "import sys; sys.path.insert(1, \"detectron2_repo/projects/PointRend\")\n",
        "from detectron2_repo.projects.PointRend import point_rend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo",
        "colab_type": "text"
      },
      "source": [
        "# Train on a custom COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_",
        "colab_type": "text"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the fruits nuts segmentation dataset](https://github.com/Tony607/mmdetection_instance_segmentation_demo)\n",
        "which only has 3 classes: data, fig, and hazelnut.\n",
        "We'll train a segmentation model from an existing model pre-trained on the COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that the COCO dataset does not have the \"data\", \"fig\" and \"hazelnut\" categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RhkndJ6JWqO",
        "colab_type": "code",
        "outputId": "6f2a102e-e96c-41a1-ec35-e19410aa0b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qg7zSVOulkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download, decompress the data\n",
        "# !wget https://github.com/Tony607/detectron2_instance_segmentation_demo/releases/download/V0.1/data.zip\n",
        "# !unzip data.zip > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW",
        "colab_type": "text"
      },
      "source": [
        "Register the fruits_nuts dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnkg1PByUjGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.data.datasets import register_coco_instances\n",
        "# register_coco_instances(\"fruits_nuts\", {}, \"./data/trainval.json\", \"./data/images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWknKqWTWIw9",
        "colab_type": "code",
        "outputId": "2f5a878f-6c4a-46a5-c7b8-1b6dc36f32d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# fruits_nuts_metadata = MetadataCatalog.get(\"fruits_nuts\")\n",
        "# dataset_dicts = DatasetCatalog.get(\"fruits_nuts\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[03/26 02:56:00 d2.data.datasets.coco]: \u001b[0mLoaded 18 images in COCO format from ./data/trainval.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-aG4sj3cV2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "下面 笔画数据集\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Retbdmc07rgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wz\", {}, \"./drive/My Drive/pic566_28class/images566.json\", \"./drive/My Drive/pic566_28class/images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttCvanr27rPN",
        "colab_type": "code",
        "outputId": "269f2976-fde0-4cbf-da52-83863f8afcf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wanzheng_metadata = MetadataCatalog.get(\"wz\")\n",
        "wanzhengdataset_dicts = DatasetCatalog.get(\"wz\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 16:02:36 d2.data.datasets.coco]: \u001b[0mLoaded 566 images in COCO format from ./drive/My Drive/pic566_28class/images566.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E",
        "colab_type": "text"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q38FZu0W37T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #坚果数据集\n",
        "# import random\n",
        "\n",
        "# for d in random.sample(dataset_dicts, 1):\n",
        "#    img = cv2.imread(d[\"file_name\"])#!!!!!!!!!!!!!!!!!！！！！！！！！！！！\n",
        "#    visualizer = Visualizer(img[:, :, ::-1], metadata=fruits_nuts_metadata, scale=0.5)\n",
        "#    vis = visualizer.draw_dataset_dict(d)\n",
        "#    cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "#    cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5JPh6Ur8FTD",
        "colab_type": "code",
        "outputId": "7d392be0-5328-4472-eb79-2f3a5e175697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#笔画数据集\n",
        "import random\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wanzheng_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(vis.get_image()[:, :, ::-1])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAvLklEQVR4nO19eZxdRZ3v71dVZ7lr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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F719F0FCB00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA",
        "colab_type": "text"
      },
      "source": [
        "Now, let's fine-tune a coco-pretrained R50-FPN Mask R-CNN model on the fruits_nuts dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.engine import DefaultTrainer\n",
        "# from detectron2.config import get_cfg\n",
        "# import os\n",
        "\n",
        "# cfg = get_cfg()\n",
        "# cfg.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "# cfg.DATASETS.TRAIN = (\"fruits_nuts\",)\n",
        "# cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "# cfg.DATALOADER.NUM_WORKERS = 2\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "# cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "# cfg.SOLVER.BASE_LR = 0.02\n",
        "# cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\n",
        "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # 3 classes (data, fig, hazelnut)\n",
        "\n",
        "# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "# trainer = DefaultTrainer(cfg)\n",
        "# trainer.resume_or_load(resume=False)\n",
        "# trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuB2wY_8kCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Add PointRend-specific config\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 28#修改POINT_HEAD.NUM_CLASSES为28 默认值为80\n",
        "\n",
        "# cfg.merge_from_file(\"./drive/My Drive/Colab Notebooks/detectron2_repo/configs/COCO-InstanceSegmentation/Base-PointRend-RCNN-FPN.yaml\")\n",
        "cfg.merge_from_file(\"./drive/My Drive/Colab Notebooks/detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"wz\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.02\n",
        "cfg.SOLVER.MAX_ITER = 100    # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE =256   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 28  # 28 classes (heng,shu....)\n",
        "# assert cfg.MODEL.ROI_HEADS.NUM_CLASSES == cfg.MODEL.POINT_HEAD.NUM_CLASSES\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVtTbR_A-WBq",
        "colab_type": "code",
        "outputId": "3335df6a-1194-4f22-da5c-c5fcac529a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#正式训练\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "       "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 16:07:40 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): PointRendROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=29, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=112, bias=True)\n",
            "    )\n",
            "    (mask_coarse_head): CoarseMaskHead(\n",
            "      (reduce_spatial_dim_conv): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (coarse_mask_fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (coarse_mask_fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (prediction): Linear(in_features=1024, out_features=1372, bias=True)\n",
            "    )\n",
            "    (mask_point_head): StandardPointHead(\n",
            "      (fc1): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc2): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc3): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (predictor): Conv1d(284, 28, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/04 16:07:40 d2.data.datasets.coco]: \u001b[0mLoaded 566 images in COCO format from ./drive/My Drive/pic566_28class/images566.json\n",
            "\u001b[32m[04/04 16:07:40 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 566 images left.\n",
            "\u001b[32m[04/04 16:07:40 d2.data.common]: \u001b[0mSerializing 566 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/04 16:07:40 d2.data.common]: \u001b[0mSerialized dataset takes 1.53 MiB\n",
            "\u001b[32m[04/04 16:07:40 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[04/04 16:07:40 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "'roi_heads.box_predictor.cls_score.weight' has shape (81, 1024) in the checkpoint but (29, 1024) in the model! Skipped.\n",
            "'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (29,) in the model! Skipped.\n",
            "'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 1024) in the checkpoint but (112, 1024) in the model! Skipped.\n",
            "'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (112,) in the model! Skipped.\n",
            "'roi_heads.mask_coarse_head.prediction.weight' has shape (3920, 1024) in the checkpoint but (1372, 1024) in the model! Skipped.\n",
            "'roi_heads.mask_coarse_head.prediction.bias' has shape (3920,) in the checkpoint but (1372,) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc1.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc2.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc3.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.predictor.weight' has shape (80, 336, 1) in the checkpoint but (28, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.predictor.bias' has shape (80,) in the checkpoint but (28,) in the model! Skipped.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 16:07:40 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/04 16:07:46 d2.utils.events]: \u001b[0m eta: 0:00:23  iter: 19  total_loss: 4.879  loss_cls: 2.714  loss_box_reg: 0.917  loss_mask: 0.691  loss_mask_point: 0.657  loss_rpn_cls: 0.044  loss_rpn_loc: 0.074  time: 0.2814  data_time: 0.0182  lr: 0.000400  max_mem: 4831M\n",
            "\u001b[32m[04/04 16:07:52 d2.utils.events]: \u001b[0m eta: 0:00:17  iter: 39  total_loss: 3.041  loss_cls: 1.013  loss_box_reg: 0.916  loss_mask: 0.582  loss_mask_point: 0.482  loss_rpn_cls: 0.023  loss_rpn_loc: 0.073  time: 0.2902  data_time: 0.0082  lr: 0.000799  max_mem: 4906M\n",
            "\u001b[32m[04/04 16:07:58 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 59  total_loss: 2.530  loss_cls: 0.830  loss_box_reg: 0.860  loss_mask: 0.460  loss_mask_point: 0.347  loss_rpn_cls: 0.015  loss_rpn_loc: 0.055  time: 0.2898  data_time: 0.0083  lr: 0.001199  max_mem: 4906M\n",
            "\u001b[32m[04/04 16:08:04 d2.utils.events]: \u001b[0m eta: 0:00:06  iter: 79  total_loss: 1.951  loss_cls: 0.615  loss_box_reg: 0.675  loss_mask: 0.341  loss_mask_point: 0.274  loss_rpn_cls: 0.008  loss_rpn_loc: 0.055  time: 0.2894  data_time: 0.0084  lr: 0.001598  max_mem: 4906M\n",
            "\u001b[32m[04/04 16:08:11 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 99  total_loss: 1.572  loss_cls: 0.491  loss_box_reg: 0.540  loss_mask: 0.253  loss_mask_point: 0.247  loss_rpn_cls: 0.008  loss_rpn_loc: 0.055  time: 0.2908  data_time: 0.0084  lr: 0.001998  max_mem: 4906M\n",
            "\u001b[32m[04/04 16:08:12 d2.engine.hooks]: \u001b[0mOverall training speed: 97 iterations in 0:00:28 (0.2938 s / it)\n",
            "\u001b[32m[04/04 16:08:12 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:30 (0:00:02 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivOaw1hgwLui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 加载预先训练好的模型权重\n",
        "del_url = \"https://dl.fbaipublicfiles.com/detectron2/PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "batch_size = 1    # just a random number\n",
        "\n",
        "\n",
        "#获取模型model   def build_model(cls, cfg):Returns: torch.nn.Module:\n",
        "PointRendModel = trainer.build_model(cfg)\n",
        " \n",
        "# # 使用预训练的权重初始化模型\n",
        "# map_location = lambda storage, loc: storage\n",
        "# if torch.cuda.is_available():\n",
        "#     map_location = None\n",
        "# PointRendModel.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
        "\n",
        "# # 将训练模式设置为falsesince we will only run the forward pass.\n",
        "# PointRendModel.train(False)\n",
        "\n",
        "\n",
        "print('------------------------完美的分割线-------------------------------------')\n",
        "\n",
        "# # 打印模型的状态字典\n",
        "# print(\"PointRendModel's state_dict:\")\n",
        "# for param_tensor in PointRendModel.state_dict():\n",
        "#     print(param_tensor, \"\\t\", PointRendModel.state_dict()[param_tensor].size())\n",
        "\n",
        "print('------------------------完美的分割线-------------------------------------')\n",
        " \n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import io\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "\n",
        "\n",
        "# 输入模型\n",
        "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
        "# 导出模型\n",
        "torch_out = torch.onnx._export(PointRendModel,             # model being run\n",
        "        x,                       # model input (or a tuple for multiple inputs)\n",
        "        \"super_resolution.onnx\", # where to save the model (can be a file or file-like object)\n",
        "        export_params=False)      # store the trained parameter weights inside the model file                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF",
        "colab_type": "text"
      },
      "source": [
        "Now, we perform inference with the trained model on the fruits_nuts dataset. First, let's create a predictor using the model we just trained:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6RCjvB9vU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "cfg.DATASETS.TEST = (\"wz\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO",
        "colab_type": "text"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM",
        "colab_type": "code",
        "outputId": "f30d3def-8d28-4404-8cca-f0db191f74a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=wanzheng_metadata, \n",
        "                   scale=0.8, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels用于实例化可视化的不同颜色模式  IMAGE_BW：与IMAGE相同，但将所有不带遮罩的区域转换为灰度。仅适用于按实例绘制蒙版预测\n",
        "    )\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "#输出预测结果masks\n",
        "    print(outputs[\"instances\"].pred_masks)\n",
        "\n",
        "#结果masks的size    \n",
        "    print(outputs[\"instances\"].pred_masks.size())#torch.Size([此图类别数, 256, 256]\n",
        "    print(outputs[\"instances\"].pred_masks.size()[0]) \n",
        "    print(outputs[\"instances\"].pred_masks[0,:,:]) \n",
        "    print(outputs[\"instances\"].pred_masks[0,:,:] + 0 ) #Python中将True/False 转换成 1/0 的方法\n",
        "    print(outputs[\"instances\"].pred_boxes)#有几个框\n",
        "    print(outputs[\"instances\"].pred_classes)#有几类\n",
        "    print(outputs[\"instances\"].pred_classes.type)#有几类\n",
        "\n",
        "#循环输出每一个mask\n",
        "num = outputs[\"instances\"].pred_masks.size()[0]\n",
        "i = 0\n",
        "if i < num:\n",
        "    mask = outputs[\"instances\"].to(\"cpu\").pred_masks[i,:,:]#第i个mask\n",
        "    mask = mask + 0  #Python中将True/False 转换成 1/0 的方法\n",
        "    mask = mask.numpy() #将Tensor或numpy.ndarray转化为PIL.Image\n",
        "    threshold = 0.5\n",
        "     \n",
        "    # m=0\n",
        "    # n=0\n",
        "    # for mask[m,:] in range(255):\n",
        "    #    for mask[m,n] in range(255):\n",
        "    #      if mask[m,n] < threshold:\n",
        "    #          mask[m,n]=0\n",
        "    #          n=n+1\n",
        "    #      else:\n",
        "    #         mask[m,n]=255\n",
        "    #         n=n+1\n",
        "    #      m=m+1 \n",
        "    # i = i + 1  \n",
        "   \n",
        " \n",
        "\n",
        "print(i)\n",
        "#展示生成的mask\n",
        "\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "\n",
        "    #如何输出单独的mask\n",
        "    "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]],\n",
            "\n",
            "        [[False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         ...,\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False],\n",
            "         [False, False, False,  ..., False, False, False]]], device='cuda:0')\n",
            "torch.Size([7, 256, 256])\n",
            "7\n",
            "tensor([[False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        ...,\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "Boxes(tensor([[111.0001, 164.1103, 239.2430, 193.1458],\n",
            "        [ 17.0041,  35.7601,  70.4903, 214.0310],\n",
            "        [149.0721, 129.9289, 191.6440, 142.5696],\n",
            "        [147.1352,  95.6963, 185.0116, 108.9679],\n",
            "        [130.0033,  74.5915, 150.6994, 142.9924],\n",
            "        [ 65.7755,  89.2272,  85.8958,  98.3355],\n",
            "        [ 64.6155, 127.2119,  83.5836, 138.2706]], device='cuda:0'))\n",
            "tensor([ 1,  3,  1,  1, 11,  1,  1], device='cuda:0')\n",
            "<built-in method type of Tensor object at 0x7f702b224c60>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-1b3a5eee93c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m              \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 256 is out of bounds for axis 0 with size 256"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLH2qPSMgOSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "#我们解释一下detectron2中内置模型使用的输入/输出格式https://detectron2.readthedocs.io/tutorials/models.html\n",
        "outputs[\"instances\"].pred_classes\n",
        "outputs[\"instances\"].pred_boxes\n",
        "print(outputs[\"instances\"].pred_masks)\n",
        "#如何可视化mask?\n",
        "#cv2_imshow(outputs[\"instances\"].pred_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bo0cypwllj",
        "colab_type": "code",
        "outputId": "306f13f0-fac2-4897-910d-07b08b30e5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "wanzheng_metadata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metadata(evaluator_type='coco', image_root='./drive/My Drive/pic566_28class/images', json_file='./drive/My Drive/pic566_28class/images566.json', name='wz', thing_classes=['piezhe', 'heng', 'hengzhewangou', 'pie', 'na', 'shuwangou', 'henggou', 'shugou', 'hengzhegou', 'hengzhezhezhegou', 'hengpie', 'shu', 'shuzhezhegou', 'dian', 'wangou', 'ti', 'shuti', 'shuzhe', 'wogou', 'hengzhe', 'xiegou', 'hengzhezhepie', 'hengzhewan', 'piedian', 'shuzhepie', 'hengxiegou', 'hengzheti', 'shuwan'], thing_dataset_id_to_contiguous_id={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvn2tueICLiE",
        "colab_type": "text"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API. This gives an AP of ~70%. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Y_TQ6YCOWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wz\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"wz\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6lYrCqLLLW",
        "colab_type": "text"
      },
      "source": [
        "## Benchmark inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxRHYcAC_Z0f",
        "colab_type": "code",
        "outputId": "1b93541d-ed1d-4fb5-ff55-1427970e38df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    outputs = predictor(im)\n",
        "    delta = time.time() - start_time\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1 / mean_delta\n",
        "print(\"Average(sec):{:.2f},fps:{:.2f}\".format(mean_delta, fps))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average(sec):0.07,fps:13.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFMOqBbWEh5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}