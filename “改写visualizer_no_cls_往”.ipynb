{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“改写visualizer no_cls 往”",
      "provenance": [],
      "collapsed_sections": [
        "jqKpPAmApFOH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiHanWang2Developer/data/blob/master/%E2%80%9C%E6%94%B9%E5%86%99visualizer_no_cls_%E5%BE%80%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR",
        "colab_type": "text"
      },
      "source": [
        "# [How to train Detectron2 with Custom COCO Datasets](https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/) | DLology\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "This notebook will help you get started with this framwork by training a instance segmentation model with your custom COCO datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bVqmEoGK4jf",
        "colab_type": "text"
      },
      "source": [
        "本文参考https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDC4G20IuIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "ababc090-fd8d-4a96-f181-ec234b2a6290"
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 24 02:12:06 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII",
        "colab_type": "text"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d001450b-dd76-40e4-94f0-d2c7a40b272e"
      },
      "source": [
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "# !pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4+cu100\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (723.9MB)\n",
            "\u001b[K     |████████████████████████████████| 723.9MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5+cu100\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 68.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5+cu100) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5+cu100) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5+cu100) (1.12.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.4.0+cu100 torchvision-0.5.0+cu100\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.18)\n",
            "Collecting pyyaml==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp36-cp36m-linux_x86_64.whl size=44074 sha256=5a53a826a4aeeda9bc924a622e35d0322cd8e70a308b2087229344443630fb5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Collecting git+https://github.com/facebookresearch/fvcore.git\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-peoh9hls\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-peoh9hls\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (1.18.4)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/81/3b/40e876afde9f5ffa1cfdce10565aba85b0dc2e067ed551dfb566cfee6d4d/yacs-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.1) (0.8.7)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.1-cp36-none-any.whl size=44852 sha256=820f95015bfac8d7a454ac9ad87270a60ab4d63c2c91a1ecaa6bce8527d09fc8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v1o7atl1/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n",
            "Successfully built fvcore\n",
            "Installing collected packages: yacs, portalocker, fvcore\n",
            "Successfully installed fvcore-0.1.1 portalocker-1.7.0 yacs-0.1.7\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-fmwu57yd\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-fmwu57yd\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (46.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.18)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267013 sha256=3c95fb47c4c64f065ce055c7796a16c8a61c9485d330313976b0acb0b1e7de51\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bywjr54v/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.0\n",
            "    Uninstalling pycocotools-2.0.0:\n",
            "      Successfully uninstalled pycocotools-2.0.0\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0+cu100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeejixTmwEmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f05e43c6-b801-44dc-c71f-425a96638663"
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
        "# clone the repo to access PointRend code. Use the same version as the installed detectron2\n",
        "!git clone --branch v0.1.1 https://github.com/facebookresearch/detectron2 detectron2_repo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
            "Collecting detectron2\n",
            "\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/detectron2-0.1.1%2Bcu100-cp36-cp36m-linux_x86_64.whl (6.2MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2MB 630kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.41.1)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2) (7.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.2.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2) (1.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2) (5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2) (1.18.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.4.7)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.9.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.6.0.post3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.29.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (46.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (3.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.4.5.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n",
            "Installing collected packages: detectron2\n",
            "Successfully installed detectron2-0.1.1+cu100\n",
            "Cloning into 'detectron2_repo'...\n",
            "remote: Enumerating objects: 4956, done.\u001b[K\n",
            "remote: Total 4956 (delta 0), reused 0 (delta 0), pack-reused 4956\u001b[K\n",
            "Receiving objects: 100% (4956/4956), 2.43 MiB | 4.18 MiB/s, done.\n",
            "Resolving deltas: 100% (3547/3547), done.\n",
            "Note: checking out '401fd04cecec16f1ed0452eb936502d5d33a23be'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# import PointRend project\n",
        "import sys; sys.path.insert(1, \"detectron2_repo/projects/PointRend\")\n",
        "from detectron2_repo.projects.PointRend import point_rend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fn57U0txcGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# master/detectron2/utils/visualizer.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "import colorsys\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "from enum import Enum, unique\n",
        "import cv2\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as mplc\n",
        "import matplotlib.figure as mplfigure\n",
        "import pycocotools.mask as mask_util\n",
        "import torch\n",
        "from fvcore.common.file_io import PathManager\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from PIL import Image\n",
        "\n",
        "from detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n",
        "\n",
        "from detectron2.utils.colormap import random_color\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n",
        "\n",
        "\n",
        "_SMALL_OBJECT_AREA_THRESH = 1000\n",
        "_LARGE_MASK_AREA_THRESH = 120000\n",
        "_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n",
        "_BLACK = (0, 0, 0)\n",
        "_RED = (1.0, 0, 0)\n",
        "\n",
        "_KEYPOINT_THRESHOLD = 0.05\n",
        "\n",
        "\n",
        "@unique\n",
        "class ColorMode(Enum):\n",
        "    \"\"\"\n",
        "    Enum of different color modes to use for instance visualizations.\n",
        "    \"\"\"\n",
        "\n",
        "    IMAGE = 0\n",
        "    \"\"\"\n",
        "    Picks a random color for every instance and overlay segmentations with low opacity.\n",
        "    \"\"\"\n",
        "    SEGMENTATION = 1\n",
        "    \"\"\"\n",
        "    Let instances of the same category have similar colors\n",
        "    (from metadata.thing_colors), and overlay them with\n",
        "    high opacity. This provides more attention on the quality of segmentation.\n",
        "    \"\"\"\n",
        "    IMAGE_BW = 2\n",
        "    \"\"\"\n",
        "    Same as IMAGE, but convert all areas without masks to gray-scale.\n",
        "    Only available for drawing per-instance mask predictions.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class GenericMask:\n",
        "    \"\"\"\n",
        "    Attribute:\n",
        "        polygons (list[ndarray]): list[ndarray]: polygons for this mask.\n",
        "            Each ndarray has format [x, y, x, y, ...]\n",
        "        mask (ndarray): a binary mask\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mask_or_polygons, height, width):\n",
        "        self._mask = self._polygons = self._has_holes = None\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        m = mask_or_polygons\n",
        "        if isinstance(m, dict):\n",
        "            # RLEs\n",
        "            assert \"counts\" in m and \"size\" in m\n",
        "            if isinstance(m[\"counts\"], list):  # uncompressed RLEs\n",
        "                h, w = m[\"size\"]\n",
        "                assert h == height and w == width\n",
        "                m = mask_util.frPyObjects(m, h, w)\n",
        "            self._mask = mask_util.decode(m)[:, :]\n",
        "            return\n",
        "\n",
        "        if isinstance(m, list):  # list[ndarray]\n",
        "            self._polygons = [np.asarray(x).reshape(-1) for x in m]\n",
        "            return\n",
        "\n",
        "        if isinstance(m, np.ndarray):  # assumed to be a binary mask\n",
        "            assert m.shape[1] != 2, m.shape\n",
        "            assert m.shape == (height, width), m.shape\n",
        "            self._mask = m.astype(\"uint8\")\n",
        "            return\n",
        "\n",
        "        raise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        if self._mask is None:\n",
        "            self._mask = self.polygons_to_mask(self._polygons)\n",
        "        return self._mask\n",
        "\n",
        "    @property\n",
        "    def polygons(self):\n",
        "        if self._polygons is None:\n",
        "            self._polygons, self._has_holes = self.mask_to_polygons(self._mask)\n",
        "        return self._polygons\n",
        "\n",
        "    @property\n",
        "    def has_holes(self):\n",
        "        if self._has_holes is None:\n",
        "            if self._mask is not None:\n",
        "                self._polygons, self._has_holes = self.mask_to_polygons(self._mask)\n",
        "            else:\n",
        "                self._has_holes = False  # if original format is polygon, does not have holes\n",
        "        return self._has_holes\n",
        "\n",
        "    def mask_to_polygons(self, mask):\n",
        "        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n",
        "        # hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n",
        "        # Internal contours (holes) are placed in hierarchy-2.\n",
        "        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\n",
        "        mask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\n",
        "        res = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
        "        hierarchy = res[-1]\n",
        "        if hierarchy is None:  # empty mask\n",
        "            return [], False\n",
        "        has_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\n",
        "        res = res[-2]\n",
        "        res = [x.flatten() for x in res]\n",
        "        res = [x for x in res if len(x) >= 6]\n",
        "        return res, has_holes\n",
        "\n",
        "    def polygons_to_mask(self, polygons):\n",
        "        rle = mask_util.frPyObjects(polygons, self.height, self.width)\n",
        "        rle = mask_util.merge(rle)\n",
        "        return mask_util.decode(rle)[:, :]\n",
        "\n",
        "    def area(self):\n",
        "        return self.mask.sum()\n",
        "\n",
        "    def bbox(self):\n",
        "        p = mask_util.frPyObjects(self.polygons, self.height, self.width)\n",
        "        p = mask_util.merge(p)\n",
        "        bbox = mask_util.toBbox(p)\n",
        "        bbox[2] += bbox[0]\n",
        "        bbox[3] += bbox[1]\n",
        "        return bbox\n",
        "\n",
        "\n",
        "class _PanopticPrediction:\n",
        "    def __init__(self, panoptic_seg, segments_info):\n",
        "        self._seg = panoptic_seg\n",
        "\n",
        "        self._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\n",
        "        segment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\n",
        "        areas = areas.numpy()\n",
        "        sorted_idxs = np.argsort(-areas)\n",
        "        self._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\n",
        "        self._seg_ids = self._seg_ids.tolist()\n",
        "        for sid, area in zip(self._seg_ids, self._seg_areas):\n",
        "            if sid in self._sinfo:\n",
        "                self._sinfo[sid][\"area\"] = float(area)\n",
        "\n",
        "    def non_empty_mask(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            (H, W) array, a mask for all pixels that have a prediction\n",
        "        \"\"\"\n",
        "        empty_ids = []\n",
        "        for id in self._seg_ids:\n",
        "            if id not in self._sinfo:\n",
        "                empty_ids.append(id)\n",
        "        if len(empty_ids) == 0:\n",
        "            return np.zeros(self._seg.shape, dtype=np.uint8)\n",
        "        assert (\n",
        "            len(empty_ids) == 1\n",
        "        ), \">1 ids corresponds to no labels. This is currently not supported\"\n",
        "        return (self._seg != empty_ids[0]).numpy().astype(np.bool)\n",
        "\n",
        "    def semantic_masks(self):\n",
        "        for sid in self._seg_ids:\n",
        "            sinfo = self._sinfo.get(sid)\n",
        "            if sinfo is None or sinfo[\"isthing\"]:\n",
        "                # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\n",
        "                continue\n",
        "            yield (self._seg == sid).numpy().astype(np.bool), sinfo\n",
        "\n",
        "    def instance_masks(self):\n",
        "        for sid in self._seg_ids:\n",
        "            sinfo = self._sinfo.get(sid)\n",
        "            if sinfo is None or not sinfo[\"isthing\"]:\n",
        "                continue\n",
        "            mask = (self._seg == sid).numpy().astype(np.bool)\n",
        "            if mask.sum() > 0:\n",
        "                yield mask, sinfo\n",
        "\n",
        "\n",
        "def _create_text_labels(classes, scores, class_names):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        classes (list[int] or None):\n",
        "        scores (list[float] or None):\n",
        "        class_names (list[str] or None):\n",
        "    Returns:\n",
        "        list[str] or None\n",
        "    \"\"\"\n",
        "    labels = None\n",
        "    if classes is not None and class_names is not None and len(class_names) > 1:\n",
        "        labels = [class_names[i] for i in classes]\n",
        "    if scores is not None:\n",
        "        if labels is None:\n",
        "            labels = [\"{:.0f}%\".format(s * 100) for s in scores]\n",
        "        else:\n",
        "            labels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\n",
        "    return labels\n",
        "\n",
        "\n",
        "class VisImage:\n",
        "    def __init__(self, img, scale=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (ndarray): an RGB image of shape (H, W, 3).\n",
        "            scale (float): scale the input image\n",
        "        \"\"\"\n",
        "        self.img = img\n",
        "        self.scale = scale\n",
        "        self.width, self.height = img.shape[1], img.shape[0]\n",
        "        self._setup_figure(img)\n",
        "\n",
        "    def _setup_figure(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Same as in :meth:`__init__()`.\n",
        "        Returns:\n",
        "            fig (matplotlib.pyplot.figure): top level container for all the image plot elements.\n",
        "            ax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n",
        "        \"\"\"\n",
        "        fig = mplfigure.Figure(frameon=False)\n",
        "        self.dpi = fig.get_dpi()\n",
        "        # add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n",
        "        # (https://github.com/matplotlib/matplotlib/issues/15363)\n",
        "        fig.set_size_inches(\n",
        "            (self.width * self.scale + 1e-2) / self.dpi,\n",
        "            (self.height * self.scale + 1e-2) / self.dpi,\n",
        "        )\n",
        "        self.canvas = FigureCanvasAgg(fig)\n",
        "        # self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\n",
        "        ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_xlim(0.0, self.width)\n",
        "        ax.set_ylim(self.height)\n",
        "\n",
        "        self.fig = fig\n",
        "        self.ax = ax\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            filepath (str): a string that contains the absolute path, including the file name, where\n",
        "                the visualized image will be saved.\n",
        "        \"\"\"\n",
        "        if filepath.lower().endswith(\".jpg\") or filepath.lower().endswith(\".png\"):\n",
        "            # faster than matplotlib's imshow\n",
        "            cv2.imwrite(filepath, self.get_image()[:, :, ::-1])\n",
        "        else:\n",
        "            # support general formats (e.g. pdf)\n",
        "            self.ax.imshow(self.img, interpolation=\"nearest\")\n",
        "            self.fig.savefig(filepath)\n",
        "\n",
        "    def get_image(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            ndarray:\n",
        "                the visualized image of shape (H, W, 3) (RGB) in uint8 type.\n",
        "                The shape is scaled w.r.t the input image using the given `scale` argument.\n",
        "        \"\"\"\n",
        "        canvas = self.canvas\n",
        "        s, (width, height) = canvas.print_to_buffer()\n",
        "        if (self.width, self.height) != (width, height):\n",
        "            img = cv2.resize(self.img, (width, height))\n",
        "        else:\n",
        "            img = self.img\n",
        "\n",
        "        # buf = io.BytesIO()  # works for cairo backend\n",
        "        # canvas.print_rgba(buf)\n",
        "        # width, height = self.width, self.height\n",
        "        # s = buf.getvalue()\n",
        "\n",
        "        buffer = np.frombuffer(s, dtype=\"uint8\")\n",
        "\n",
        "        # imshow is slow. blend manually (still quite slow)\n",
        "        img_rgba = buffer.reshape(height, width, 4)\n",
        "        rgb, alpha = np.split(img_rgba, [3], axis=2)\n",
        "\n",
        "        try:\n",
        "            import numexpr as ne  # fuse them with numexpr\n",
        "\n",
        "            visualized_image = ne.evaluate(\"img * (1 - alpha / 255.0) + rgb * (alpha / 255.0)\")\n",
        "        except ImportError:\n",
        "            alpha = alpha.astype(\"float32\") / 255.0\n",
        "            visualized_image = img * (1 - alpha) + rgb * alpha\n",
        "\n",
        "        visualized_image = visualized_image.astype(\"uint8\")\n",
        "\n",
        "        return visualized_image\n",
        "\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self, img_rgb, metadata, scale=1.0, instance_mode=ColorMode.IMAGE):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_rgb: a numpy array of shape (H, W, C), where H and W correspond to\n",
        "                the height and width of the image respectively. C is the number of\n",
        "                color channels. The image is required to be in RGB format since that\n",
        "                is a requirement of the Matplotlib library. The image is also expected\n",
        "                to be in the range [0, 255].\n",
        "            metadata (MetadataCatalog): image metadata.\n",
        "        \"\"\"\n",
        "        self.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\n",
        "        self.metadata = metadata\n",
        "        self.output = VisImage(self.img, scale=scale)\n",
        "        self.cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "        # too small texts are useless, therefore clamp to 9\n",
        "        self._default_font_size = max(\n",
        "            np.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n",
        "        )\n",
        "        self._instance_mode = instance_mode\n",
        "\n",
        "\n",
        "    def draw_instance_predictions_no_cls(self, predictions):\n",
        "        \"\"\"\n",
        "        Draw instance-level prediction results on an image.\n",
        "        Args:\n",
        "            predictions (Instances): the output of an instance detection/segmentation\n",
        "                model. Following fields will be used to draw:\n",
        "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n",
        "        scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n",
        "        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
        "        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n",
        "\n",
        "        if predictions.has(\"pred_masks\"):\n",
        "            masks = np.asarray(predictions.pred_masks)\n",
        "            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n",
        "        else:\n",
        "            masks = None\n",
        "\n",
        "        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "            colors = [\n",
        "                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n",
        "            ]\n",
        "            alpha = 0.8\n",
        "        else:\n",
        "            colors = None\n",
        "            alpha = 0.5\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            self.output.img = self._create_grayscale_image(\n",
        "                (predictions.pred_masks.any(dim=0) > 0).numpy()\n",
        "            )\n",
        "            alpha = 0.3\n",
        "\n",
        "        # self.overlay_instances(\n",
        "        #     masks=masks,\n",
        "        #     boxes=boxes,\n",
        "        #     labels=labels,\n",
        "        #     keypoints=keypoints,\n",
        "        #     assigned_colors=colors,\n",
        "        #     alpha=alpha,\n",
        "        # )\n",
        "        self.overlay_instances_no_cls(\n",
        "            masks=masks,\n",
        "            boxes=boxes,\n",
        "            labels=labels,\n",
        "            keypoints=keypoints,\n",
        "            assigned_colors=colors,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "    \n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def draw_instance_predictions(self, predictions):\n",
        "        \"\"\"\n",
        "        Draw instance-level prediction results on an image.\n",
        "        Args:\n",
        "            predictions (Instances): the output of an instance detection/segmentation\n",
        "                model. Following fields will be used to draw:\n",
        "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n",
        "        scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n",
        "        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
        "        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n",
        "\n",
        "        if predictions.has(\"pred_masks\"):\n",
        "            masks = np.asarray(predictions.pred_masks)\n",
        "            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n",
        "        else:\n",
        "            masks = None\n",
        "\n",
        "        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "            colors = [\n",
        "                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n",
        "            ]\n",
        "            alpha = 0.8\n",
        "        else:\n",
        "            colors = None\n",
        "            alpha = 0.5\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            self.output.img = self._create_grayscale_image(\n",
        "                (predictions.pred_masks.any(dim=0) > 0).numpy()\n",
        "            )\n",
        "            alpha = 0.3\n",
        "\n",
        "        self.overlay_instances(\n",
        "            masks=masks,\n",
        "            boxes=boxes,\n",
        "            labels=labels,\n",
        "            keypoints=keypoints,\n",
        "            assigned_colors=colors,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "    \n",
        "        return self.output\n",
        "    def draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n",
        "        \"\"\"\n",
        "        Draw semantic segmentation predictions/labels.\n",
        "        Args:\n",
        "            sem_seg (Tensor or ndarray): the segmentation of shape (H, W).\n",
        "                Each value is the integer label of the pixel.\n",
        "            area_threshold (int): segments with less than `area_threshold` are not drawn.\n",
        "            alpha (float): the larger it is, the more opaque the segmentations are.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        if isinstance(sem_seg, torch.Tensor):\n",
        "            sem_seg = sem_seg.numpy()\n",
        "        labels, areas = np.unique(sem_seg, return_counts=True)\n",
        "        sorted_idxs = np.argsort(-areas).tolist()\n",
        "        labels = labels[sorted_idxs]\n",
        "        for label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\n",
        "            try:\n",
        "                mask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\n",
        "            except (AttributeError, IndexError):\n",
        "                mask_color = None\n",
        "\n",
        "            binary_mask = (sem_seg == label).astype(np.uint8)\n",
        "            text = self.metadata.stuff_classes[label]\n",
        "            self.draw_binary_mask(\n",
        "                binary_mask,\n",
        "                color=mask_color,\n",
        "                edge_color=_OFF_WHITE,\n",
        "                text=text,\n",
        "                alpha=alpha,\n",
        "                area_threshold=area_threshold,\n",
        "            )\n",
        "        return self.output\n",
        "\n",
        "    def draw_panoptic_seg_predictions(\n",
        "        self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Draw panoptic prediction results on an image.\n",
        "        Args:\n",
        "            panoptic_seg (Tensor): of shape (height, width) where the values are ids for each\n",
        "                segment.\n",
        "            segments_info (list[dict]): Describe each segment in `panoptic_seg`.\n",
        "                Each dict contains keys \"id\", \"category_id\", \"isthing\".\n",
        "            area_threshold (int): stuff segments with less than `area_threshold` are not drawn.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        pred = _PanopticPrediction(panoptic_seg, segments_info)\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            self.output.img = self._create_grayscale_image(pred.non_empty_mask())\n",
        "\n",
        "        # draw mask for all semantic segments first i.e. \"stuff\"\n",
        "        for mask, sinfo in pred.semantic_masks():\n",
        "            category_idx = sinfo[\"category_id\"]\n",
        "            try:\n",
        "                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\n",
        "            except AttributeError:\n",
        "                mask_color = None\n",
        "\n",
        "            text = self.metadata.stuff_classes[category_idx]\n",
        "            self.draw_binary_mask(\n",
        "                mask,\n",
        "                color=mask_color,\n",
        "                edge_color=_OFF_WHITE,\n",
        "                text=text,\n",
        "                alpha=alpha,\n",
        "                area_threshold=area_threshold,\n",
        "            )\n",
        "\n",
        "        # draw mask for all instances second\n",
        "        all_instances = list(pred.instance_masks())\n",
        "        if len(all_instances) == 0:\n",
        "            return self.output\n",
        "        masks, sinfo = list(zip(*all_instances))\n",
        "        category_ids = [x[\"category_id\"] for x in sinfo]\n",
        "\n",
        "        try:\n",
        "            scores = [x[\"score\"] for x in sinfo]\n",
        "        except KeyError:\n",
        "            scores = None\n",
        "        labels = _create_text_labels(category_ids, scores, self.metadata.thing_classes)\n",
        "\n",
        "        try:\n",
        "            colors = [random_color(rgb=True, maximum=1) for k in category_ids]\n",
        "        except AttributeError:\n",
        "            colors = None\n",
        "        self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def draw_dataset_dict_no_cls(self, dic):\n",
        "        \"\"\"\n",
        "        Draw annotations/segmentaions in Detectron2 Dataset format.\n",
        "        Args:\n",
        "            dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        annos = dic.get(\"annotations\", None)\n",
        "        if annos:\n",
        "            if \"segmentation\" in annos[0]:\n",
        "                masks = [x[\"segmentation\"] for x in annos]\n",
        "            else:\n",
        "                masks = None\n",
        "            if \"keypoints\" in annos[0]:\n",
        "                keypts = [x[\"keypoints\"] for x in annos]\n",
        "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
        "            else:\n",
        "                keypts = None\n",
        "\n",
        "            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n",
        "\n",
        "            labels = [x[\"category_id\"] for x in annos]\n",
        "            colors = None\n",
        "            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "                colors = [\n",
        "                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n",
        "                ]\n",
        "            names = self.metadata.get(\"thing_classes\", None)\n",
        "            if names:\n",
        "                labels = [names[i] for i in labels]\n",
        "            labels = [\n",
        "                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
        "                for i, a in zip(labels, annos)\n",
        "            ]\n",
        "            self.overlay_instances_no_cls(\n",
        "                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n",
        "            )\n",
        "\n",
        "        sem_seg = dic.get(\"sem_seg\", None)\n",
        "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
        "            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n",
        "                sem_seg = Image.open(f)\n",
        "                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n",
        "        if sem_seg is not None:\n",
        "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def draw_dataset_dict(self, dic):\n",
        "        \"\"\"\n",
        "        Draw annotations/segmentaions in Detectron2 Dataset format.\n",
        "        Args:\n",
        "            dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        annos = dic.get(\"annotations\", None)\n",
        "        if annos:\n",
        "            if \"segmentation\" in annos[0]:\n",
        "                masks = [x[\"segmentation\"] for x in annos]\n",
        "            else:\n",
        "                masks = None\n",
        "            if \"keypoints\" in annos[0]:\n",
        "                keypts = [x[\"keypoints\"] for x in annos]\n",
        "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
        "            else:\n",
        "                keypts = None\n",
        "\n",
        "            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n",
        "\n",
        "            labels = [x[\"category_id\"] for x in annos]\n",
        "            colors = None\n",
        "            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "                colors = [\n",
        "                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n",
        "                ]\n",
        "            names = self.metadata.get(\"thing_classes\", None)\n",
        "            if names:\n",
        "                labels = [names[i] for i in labels]\n",
        "            labels = [\n",
        "                \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
        "                for i, a in zip(labels, annos)\n",
        "            ]\n",
        "            self.overlay_instances(\n",
        "                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n",
        "            )\n",
        "\n",
        "        sem_seg = dic.get(\"sem_seg\", None)\n",
        "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
        "            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n",
        "                sem_seg = Image.open(f)\n",
        "                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n",
        "        if sem_seg is not None:\n",
        "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
        "        return self.output\n",
        "\n",
        "    def overlay_instances_no_cls(\n",
        "        self,\n",
        "        *,\n",
        "        boxes=None,\n",
        "        labels=None,\n",
        "        masks=None,\n",
        "        keypoints=None,\n",
        "        assigned_colors=None,\n",
        "        alpha=0.5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\n",
        "                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\n",
        "                or a :class:`RotatedBoxes`,\n",
        "                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\n",
        "                for the N objects in a single image,\n",
        "            labels (list[str]): the text to be displayed for each instance.\n",
        "            masks (masks-like object): Supported types are:\n",
        "                * :class:`detectron2.structures.PolygonMasks`,\n",
        "                  :class:`detectron2.structures.BitMasks`.\n",
        "                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.\n",
        "                  The first level of the list corresponds to individual instances. The second\n",
        "                  level to all the polygon that compose the instance, and the third level\n",
        "                  to the polygon coordinates. The third level should have the format of\n",
        "                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n",
        "                * list[ndarray]: each ndarray is a binary mask of shape (H, W).\n",
        "                * list[dict]: each dict is a COCO-style RLE.\n",
        "            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\n",
        "                where the N is the number of instances and K is the number of keypoints.\n",
        "                The last dimension corresponds to (x, y, visibility or score).\n",
        "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
        "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
        "                for full list of formats that the colors are accepted in.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        num_instances = None\n",
        "        if boxes is not None:\n",
        "            boxes = self._convert_boxes(boxes)\n",
        "            num_instances = len(boxes)\n",
        "        if masks is not None:\n",
        "            masks = self._convert_masks(masks)\n",
        "            if num_instances:\n",
        "                assert len(masks) == num_instances\n",
        "            else:\n",
        "                num_instances = len(masks)\n",
        "        if keypoints is not None:\n",
        "            if num_instances:\n",
        "                assert len(keypoints) == num_instances\n",
        "            else:\n",
        "                num_instances = len(keypoints)\n",
        "            keypoints = self._convert_keypoints(keypoints)\n",
        "        if labels is not None:\n",
        "            assert len(labels) == num_instances\n",
        "        if assigned_colors is None:\n",
        "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "        if num_instances == 0:\n",
        "            return self.output\n",
        "        if boxes is not None and boxes.shape[1] == 5:\n",
        "            return self.overlay_rotated_instances(\n",
        "                boxes=boxes, labels=labels, assigned_colors=assigned_colors\n",
        "            )\n",
        "\n",
        "        # Display in largest to smallest order to reduce occlusion.\n",
        "        areas = None\n",
        "        if boxes is not None:\n",
        "            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
        "        elif masks is not None:\n",
        "            areas = np.asarray([x.area() for x in masks])\n",
        "\n",
        "        if areas is not None:\n",
        "            sorted_idxs = np.argsort(-areas).tolist()\n",
        "            # Re-order overlapped instances in descending order.\n",
        "            boxes = boxes[sorted_idxs] if boxes is not None else None\n",
        "            labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
        "            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
        "            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
        "            keypoints = keypoints[sorted_idxs] if keypoints is not None else None\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            color = assigned_colors[i]\n",
        "            if boxes is not None:\n",
        "                self.draw_box(boxes[i], edge_color=color)\n",
        "\n",
        "            if masks is not None:\n",
        "                for segment in masks[i].polygons:\n",
        "                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n",
        "\n",
        "            # if labels is not None:\n",
        "            #     # first get a box\n",
        "            #     if boxes is not None:\n",
        "            #         x0, y0, x1, y1 = boxes[i]\n",
        "            #         text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
        "            #         horiz_align = \"left\"\n",
        "            #     elif masks is not None:\n",
        "            #         x0, y0, x1, y1 = masks[i].bbox()\n",
        "\n",
        "            #         # draw text in the center (defined by median) when box is not drawn\n",
        "            #         # median is less sensitive to outliers.\n",
        "            #         text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
        "            #         horiz_align = \"center\"\n",
        "            #     else:\n",
        "            #         continue  # drawing the box confidence for keypoints isn't very useful.\n",
        "            #     # for small objects, draw text at the side to avoid occlusion\n",
        "            #     instance_area = (y1 - y0) * (x1 - x0)\n",
        "            #     if (\n",
        "            #         instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n",
        "            #         or y1 - y0 < 40 * self.output.scale\n",
        "            #     ):\n",
        "            #         if y1 >= self.output.height - 5:\n",
        "            #             text_pos = (x1, y0)\n",
        "            #         else:\n",
        "            #             text_pos = (x0, y1)\n",
        "\n",
        "            #     height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n",
        "            #     lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "            #     font_size = (\n",
        "            #         np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
        "            #         * 0.5\n",
        "            #         * self._default_font_size\n",
        "            #     )\n",
        "            #     self.draw_text(\n",
        "            #         labels[i],\n",
        "            #         text_pos,\n",
        "            #         color=lighter_color,\n",
        "            #         horizontal_alignment=horiz_align,\n",
        "            #         font_size=font_size,\n",
        "            #     )\n",
        "\n",
        "        # draw keypoints\n",
        "        if keypoints is not None:\n",
        "            for keypoints_per_instance in keypoints:\n",
        "                self.draw_and_connect_keypoints(keypoints_per_instance)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def overlay_instances(\n",
        "        self,\n",
        "        *,\n",
        "        boxes=None,\n",
        "        labels=None,\n",
        "        masks=None,\n",
        "        keypoints=None,\n",
        "        assigned_colors=None,\n",
        "        alpha=0.5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\n",
        "                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\n",
        "                or a :class:`RotatedBoxes`,\n",
        "                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\n",
        "                for the N objects in a single image,\n",
        "            labels (list[str]): the text to be displayed for each instance.\n",
        "            masks (masks-like object): Supported types are:\n",
        "                * :class:`detectron2.structures.PolygonMasks`,\n",
        "                  :class:`detectron2.structures.BitMasks`.\n",
        "                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.\n",
        "                  The first level of the list corresponds to individual instances. The second\n",
        "                  level to all the polygon that compose the instance, and the third level\n",
        "                  to the polygon coordinates. The third level should have the format of\n",
        "                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n",
        "                * list[ndarray]: each ndarray is a binary mask of shape (H, W).\n",
        "                * list[dict]: each dict is a COCO-style RLE.\n",
        "            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\n",
        "                where the N is the number of instances and K is the number of keypoints.\n",
        "                The last dimension corresponds to (x, y, visibility or score).\n",
        "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
        "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
        "                for full list of formats that the colors are accepted in.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        num_instances = None\n",
        "        if boxes is not None:\n",
        "            boxes = self._convert_boxes(boxes)\n",
        "            num_instances = len(boxes)\n",
        "        if masks is not None:\n",
        "            masks = self._convert_masks(masks)\n",
        "            if num_instances:\n",
        "                assert len(masks) == num_instances\n",
        "            else:\n",
        "                num_instances = len(masks)\n",
        "        if keypoints is not None:\n",
        "            if num_instances:\n",
        "                assert len(keypoints) == num_instances\n",
        "            else:\n",
        "                num_instances = len(keypoints)\n",
        "            keypoints = self._convert_keypoints(keypoints)\n",
        "        if labels is not None:\n",
        "            assert len(labels) == num_instances\n",
        "        if assigned_colors is None:\n",
        "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "        if num_instances == 0:\n",
        "            return self.output\n",
        "        if boxes is not None and boxes.shape[1] == 5:\n",
        "            return self.overlay_rotated_instances(\n",
        "                boxes=boxes, labels=labels, assigned_colors=assigned_colors\n",
        "            )\n",
        "\n",
        "        # Display in largest to smallest order to reduce occlusion.\n",
        "        areas = None\n",
        "        if boxes is not None:\n",
        "            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
        "        elif masks is not None:\n",
        "            areas = np.asarray([x.area() for x in masks])\n",
        "\n",
        "        if areas is not None:\n",
        "            sorted_idxs = np.argsort(-areas).tolist()\n",
        "            # Re-order overlapped instances in descending order.\n",
        "            boxes = boxes[sorted_idxs] if boxes is not None else None\n",
        "            labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
        "            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
        "            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
        "            keypoints = keypoints[sorted_idxs] if keypoints is not None else None\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            color = assigned_colors[i]\n",
        "            if boxes is not None:\n",
        "                self.draw_box(boxes[i], edge_color=color)\n",
        "\n",
        "            if masks is not None:\n",
        "                for segment in masks[i].polygons:\n",
        "                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n",
        "\n",
        "            if labels is not None:\n",
        "                # first get a box\n",
        "                if boxes is not None:\n",
        "                    x0, y0, x1, y1 = boxes[i]\n",
        "                    text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
        "                    horiz_align = \"left\"\n",
        "                elif masks is not None:\n",
        "                    x0, y0, x1, y1 = masks[i].bbox()\n",
        "\n",
        "                    # draw text in the center (defined by median) when box is not drawn\n",
        "                    # median is less sensitive to outliers.\n",
        "                    text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
        "                    horiz_align = \"center\"\n",
        "                else:\n",
        "                    continue  # drawing the box confidence for keypoints isn't very useful.\n",
        "                # for small objects, draw text at the side to avoid occlusion\n",
        "                instance_area = (y1 - y0) * (x1 - x0)\n",
        "                if (\n",
        "                    instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n",
        "                    or y1 - y0 < 40 * self.output.scale\n",
        "                ):\n",
        "                    if y1 >= self.output.height - 5:\n",
        "                        text_pos = (x1, y0)\n",
        "                    else:\n",
        "                        text_pos = (x0, y1)\n",
        "\n",
        "                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n",
        "                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "                font_size = (\n",
        "                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
        "                    * 0.5\n",
        "                    * self._default_font_size\n",
        "                )\n",
        "                self.draw_text(\n",
        "                    labels[i],\n",
        "                    text_pos,\n",
        "                    color=lighter_color,\n",
        "                    horizontal_alignment=horiz_align,\n",
        "                    font_size=font_size,\n",
        "                )\n",
        "\n",
        "        # draw keypoints\n",
        "        if keypoints is not None:\n",
        "            for keypoints_per_instance in keypoints:\n",
        "                self.draw_and_connect_keypoints(keypoints_per_instance)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (ndarray): an Nx5 numpy array of\n",
        "                (x_center, y_center, width, height, angle_degrees) format\n",
        "                for the N objects in a single image.\n",
        "            labels (list[str]): the text to be displayed for each instance.\n",
        "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
        "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
        "                for full list of formats that the colors are accepted in.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "\n",
        "        num_instances = len(boxes)\n",
        "\n",
        "        if assigned_colors is None:\n",
        "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "        if num_instances == 0:\n",
        "            return self.output\n",
        "\n",
        "        # Display in largest to smallest order to reduce occlusion.\n",
        "        if boxes is not None:\n",
        "            areas = boxes[:, 2] * boxes[:, 3]\n",
        "\n",
        "        sorted_idxs = np.argsort(-areas).tolist()\n",
        "        # Re-order overlapped instances in descending order.\n",
        "        boxes = boxes[sorted_idxs]\n",
        "        labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
        "        colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            self.draw_rotated_box_with_label(\n",
        "                boxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n",
        "            )\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def draw_and_connect_keypoints(self, keypoints):\n",
        "        \"\"\"\n",
        "        Draws keypoints of an instance and follows the rules for keypoint connections\n",
        "        to draw lines between appropriate keypoints. This follows color heuristics for\n",
        "        line color.\n",
        "        Args:\n",
        "            keypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\n",
        "                and the last dimension corresponds to (x, y, probability).\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        visible = {}\n",
        "        keypoint_names = self.metadata.get(\"keypoint_names\")\n",
        "        for idx, keypoint in enumerate(keypoints):\n",
        "            # draw keypoint\n",
        "            x, y, prob = keypoint\n",
        "            if prob > _KEYPOINT_THRESHOLD:\n",
        "                self.draw_circle((x, y), color=_RED)\n",
        "                if keypoint_names:\n",
        "                    keypoint_name = keypoint_names[idx]\n",
        "                    visible[keypoint_name] = (x, y)\n",
        "\n",
        "        if self.metadata.get(\"keypoint_connection_rules\"):\n",
        "            for kp0, kp1, color in self.metadata.keypoint_connection_rules:\n",
        "                if kp0 in visible and kp1 in visible:\n",
        "                    x0, y0 = visible[kp0]\n",
        "                    x1, y1 = visible[kp1]\n",
        "                    color = tuple(x / 255.0 for x in color)\n",
        "                    self.draw_line([x0, x1], [y0, y1], color=color)\n",
        "\n",
        "        # draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n",
        "        # Note that this strategy is specific to person keypoints.\n",
        "        # For other keypoints, it should just do nothing\n",
        "        try:\n",
        "            ls_x, ls_y = visible[\"left_shoulder\"]\n",
        "            rs_x, rs_y = visible[\"right_shoulder\"]\n",
        "            mid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\n",
        "        except KeyError:\n",
        "            pass\n",
        "        else:\n",
        "            # draw line from nose to mid-shoulder\n",
        "            nose_x, nose_y = visible.get(\"nose\", (None, None))\n",
        "            if nose_x is not None:\n",
        "                self.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\n",
        "\n",
        "            try:\n",
        "                # draw line from mid-shoulder to mid-hip\n",
        "                lh_x, lh_y = visible[\"left_hip\"]\n",
        "                rh_x, rh_y = visible[\"right_hip\"]\n",
        "            except KeyError:\n",
        "                pass\n",
        "            else:\n",
        "                mid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\n",
        "                self.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\n",
        "        return self.output\n",
        "\n",
        "    \"\"\"\n",
        "    Primitive drawing functions:\n",
        "    \"\"\"\n",
        "\n",
        "    def draw_text(\n",
        "        self,\n",
        "        text,\n",
        "        position,\n",
        "        *,\n",
        "        font_size=None,\n",
        "        color=\"g\",\n",
        "        horizontal_alignment=\"center\",\n",
        "        rotation=0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text (str): class label\n",
        "            position (tuple): a tuple of the x and y coordinates to place text on image.\n",
        "            font_size (int, optional): font of the text. If not provided, a font size\n",
        "                proportional to the image width is calculated and used.\n",
        "            color: color of the text. Refer to `matplotlib.colors` for full list\n",
        "                of formats that are accepted.\n",
        "            horizontal_alignment (str): see `matplotlib.text.Text`\n",
        "            rotation: rotation angle in degrees CCW\n",
        "        Returns:\n",
        "            output (VisImage): image object with text drawn.\n",
        "        \"\"\"\n",
        "        if not font_size:\n",
        "            font_size = self._default_font_size\n",
        "\n",
        "        # since the text background is dark, we don't want the text to be dark\n",
        "        color = np.maximum(list(mplc.to_rgb(color)), 0.2)\n",
        "        color[np.argmax(color)] = max(0.8, np.max(color))\n",
        "\n",
        "        x, y = position\n",
        "        self.output.ax.text(\n",
        "            x,\n",
        "            y,\n",
        "            text,\n",
        "            size=font_size * self.output.scale,\n",
        "            family=\"sans-serif\",\n",
        "            bbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\n",
        "            verticalalignment=\"top\",\n",
        "            horizontalalignment=horizontal_alignment,\n",
        "            color=color,\n",
        "            zorder=10,\n",
        "            rotation=rotation,\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n",
        "                are the coordinates of the image's top left corner. x1 and y1 are the\n",
        "                coordinates of the image's bottom right corner.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
        "                for full list of formats that are accepted.\n",
        "            line_style (string): the string to use to create the outline of the boxes.\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        x0, y0, x1, y1 = box_coord\n",
        "        width = x1 - x0\n",
        "        height = y1 - y0\n",
        "\n",
        "        linewidth = max(self._default_font_size / 4, 1)\n",
        "\n",
        "        self.output.ax.add_patch(\n",
        "            mpl.patches.Rectangle(\n",
        "                (x0, y0),\n",
        "                width,\n",
        "                height,\n",
        "                fill=False,\n",
        "                edgecolor=edge_color,\n",
        "                linewidth=linewidth * self.output.scale,\n",
        "                alpha=alpha,\n",
        "                linestyle=line_style,\n",
        "            )\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_rotated_box_with_label(\n",
        "        self, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\n",
        "                where cnt_x and cnt_y are the center coordinates of the box.\n",
        "                w and h are the width and height of the box. angle represents how\n",
        "                many degrees the box is rotated CCW with regard to the 0-degree box.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
        "                for full list of formats that are accepted.\n",
        "            line_style (string): the string to use to create the outline of the boxes.\n",
        "            label (string): label for rotated box. It will not be rendered when set to None.\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        cnt_x, cnt_y, w, h, angle = rotated_box\n",
        "        area = w * h\n",
        "        # use thinner lines when the box is small\n",
        "        linewidth = self._default_font_size / (\n",
        "            6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n",
        "        )\n",
        "\n",
        "        theta = angle * math.pi / 180.0\n",
        "        c = math.cos(theta)\n",
        "        s = math.sin(theta)\n",
        "        rect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n",
        "        # x: left->right ; y: top->down\n",
        "        rotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\n",
        "        for k in range(4):\n",
        "            j = (k + 1) % 4\n",
        "            self.draw_line(\n",
        "                [rotated_rect[k][0], rotated_rect[j][0]],\n",
        "                [rotated_rect[k][1], rotated_rect[j][1]],\n",
        "                color=edge_color,\n",
        "                linestyle=\"--\" if k == 1 else line_style,\n",
        "                linewidth=linewidth,\n",
        "            )\n",
        "\n",
        "        if label is not None:\n",
        "            text_pos = rotated_rect[1]  # topleft corner\n",
        "\n",
        "            height_ratio = h / np.sqrt(self.output.height * self.output.width)\n",
        "            label_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\n",
        "            font_size = (\n",
        "                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n",
        "            )\n",
        "            self.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def draw_circle(self, circle_coord, color, radius=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            circle_coord (list(int) or tuple(int)): contains the x and y coordinates\n",
        "                of the center of the circle.\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            radius (int): radius of the circle.\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        x, y = circle_coord\n",
        "        self.output.ax.add_patch(\n",
        "            mpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_data (list[int]): a list containing x values of all the points being drawn.\n",
        "                Length of list should match the length of y_data.\n",
        "            y_data (list[int]): a list containing y values of all the points being drawn.\n",
        "                Length of list should match the length of x_data.\n",
        "            color: color of the line. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            linestyle: style of the line. Refer to `matplotlib.lines.Line2D`\n",
        "                for a full list of formats that are accepted.\n",
        "            linewidth (float or None): width of the line. When it's None,\n",
        "                a default value will be computed and used.\n",
        "        Returns:\n",
        "            output (VisImage): image object with line drawn.\n",
        "        \"\"\"\n",
        "        if linewidth is None:\n",
        "            linewidth = self._default_font_size / 3\n",
        "        linewidth = max(linewidth, 1)\n",
        "        self.output.ax.add_line(\n",
        "            mpl.lines.Line2D(\n",
        "                x_data,\n",
        "                y_data,\n",
        "                linewidth=linewidth * self.output.scale,\n",
        "                color=color,\n",
        "                linestyle=linestyle,\n",
        "            )\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_binary_mask(\n",
        "        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=4096\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\n",
        "                W is the image width. Each value in the array is either a 0 or 1 value of uint8\n",
        "                type.\n",
        "            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted. If None, will pick a random color.\n",
        "            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
        "                full list of formats that are accepted.\n",
        "            text (str): if None, will be drawn in the object's center of mass.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            area_threshold (float): a connected component small than this will not be shown.\n",
        "        Returns:\n",
        "            output (VisImage): image object with mask drawn.\n",
        "        \"\"\"\n",
        "        if color is None:\n",
        "            color = random_color(rgb=True, maximum=1)\n",
        "        if area_threshold is None:\n",
        "            area_threshold = 4096\n",
        "\n",
        "        has_valid_segment = False\n",
        "        binary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\n",
        "        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n",
        "        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n",
        "\n",
        "        if not mask.has_holes:\n",
        "            # draw polygons for regular masks\n",
        "            for segment in mask.polygons:\n",
        "                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\n",
        "                if area < area_threshold:\n",
        "                    continue\n",
        "                has_valid_segment = True\n",
        "                segment = segment.reshape(-1, 2)\n",
        "                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\n",
        "        else:\n",
        "            rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n",
        "            rgba[:, :, :3] = color\n",
        "            rgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\n",
        "            has_valid_segment = True\n",
        "            self.output.ax.imshow(rgba)\n",
        "\n",
        "        if text is not None and has_valid_segment:\n",
        "            # TODO sometimes drawn on wrong objects. the heuristics here can improve.\n",
        "            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "            _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\n",
        "            largest_component_id = np.argmax(stats[1:, -1]) + 1\n",
        "\n",
        "            # draw text on the largest component, as well as other very large components.\n",
        "            for cid in range(1, _num_cc):\n",
        "                if cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n",
        "                    # median is more stable than centroid\n",
        "                    # center = centroids[largest_component_id]\n",
        "                    center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\n",
        "                    self.draw_text(text, center, color=lighter_color)\n",
        "        return self.output\n",
        "\n",
        "    def draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            segment: numpy array of shape Nx2, containing all the points in the polygon.\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
        "                full list of formats that are accepted. If not provided, a darker shade\n",
        "                of the polygon color will be used instead.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "        Returns:\n",
        "            output (VisImage): image object with polygon drawn.\n",
        "        \"\"\"\n",
        "        if edge_color is None:\n",
        "            # make edge color darker than the polygon color\n",
        "            if alpha > 0.8:\n",
        "                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n",
        "            else:\n",
        "                edge_color = color\n",
        "        edge_color = mplc.to_rgb(edge_color) + (1,)\n",
        "\n",
        "        polygon = mpl.patches.Polygon(\n",
        "            segment,\n",
        "            fill=True,\n",
        "            facecolor=mplc.to_rgb(color) + (alpha,),\n",
        "            edgecolor=edge_color,\n",
        "            linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n",
        "        )\n",
        "        self.output.ax.add_patch(polygon)\n",
        "        return self.output\n",
        "\n",
        "    \"\"\"\n",
        "    Internal methods:\n",
        "    \"\"\"\n",
        "\n",
        "    def _jitter(self, color):\n",
        "        \"\"\"\n",
        "        Randomly modifies given color to produce a slightly different color than the color given.\n",
        "        Args:\n",
        "            color (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\n",
        "                picked. The values in the list are in the [0.0, 1.0] range.\n",
        "        Returns:\n",
        "            jittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\n",
        "                color after being jittered. The values in the list are in the [0.0, 1.0] range.\n",
        "        \"\"\"\n",
        "        color = mplc.to_rgb(color)\n",
        "        vec = np.random.rand(3)\n",
        "        # better to do it in another color space\n",
        "        vec = vec / np.linalg.norm(vec) * 0.5\n",
        "        res = np.clip(vec + color, 0, 1)\n",
        "        return tuple(res)\n",
        "\n",
        "    def _create_grayscale_image(self, mask=None):\n",
        "        \"\"\"\n",
        "        Create a grayscale version of the original image.\n",
        "        The colors in masked area, if given, will be kept.\n",
        "        \"\"\"\n",
        "        img_bw = self.img.astype(\"f4\").mean(axis=2)\n",
        "        img_bw = np.stack([img_bw] * 3, axis=2)\n",
        "        if mask is not None:\n",
        "            img_bw[mask] = self.img[mask]\n",
        "        return img_bw\n",
        "\n",
        "    def _change_color_brightness(self, color, brightness_factor):\n",
        "        \"\"\"\n",
        "        Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n",
        "        less or more saturation than the original color.\n",
        "        Args:\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n",
        "                0 will correspond to no change, a factor in [-1.0, 0) range will result in\n",
        "                a darker color and a factor in (0, 1.0] range will result in a lighter color.\n",
        "        Returns:\n",
        "            modified_color (tuple[double]): a tuple containing the RGB values of the\n",
        "                modified color. Each value in the tuple is in the [0.0, 1.0] range.\n",
        "        \"\"\"\n",
        "        assert brightness_factor >= -1.0 and brightness_factor <= 1.0\n",
        "        color = mplc.to_rgb(color)\n",
        "        polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\n",
        "        modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\n",
        "        modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\n",
        "        modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\n",
        "        modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\n",
        "        return modified_color\n",
        "\n",
        "    def _convert_boxes(self, boxes):\n",
        "        \"\"\"\n",
        "        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n",
        "        \"\"\"\n",
        "        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n",
        "            return boxes.tensor.numpy()\n",
        "        else:\n",
        "            return np.asarray(boxes)\n",
        "\n",
        "    def _convert_masks(self, masks_or_polygons):\n",
        "        \"\"\"\n",
        "        Convert different format of masks or polygons to a tuple of masks and polygons.\n",
        "        Returns:\n",
        "            list[GenericMask]:\n",
        "        \"\"\"\n",
        "\n",
        "        m = masks_or_polygons\n",
        "        if isinstance(m, PolygonMasks):\n",
        "            m = m.polygons\n",
        "        if isinstance(m, BitMasks):\n",
        "            m = m.tensor.numpy()\n",
        "        if isinstance(m, torch.Tensor):\n",
        "            m = m.numpy()\n",
        "        ret = []\n",
        "        for x in m:\n",
        "            if isinstance(x, GenericMask):\n",
        "                ret.append(x)\n",
        "            else:\n",
        "                ret.append(GenericMask(x, self.output.height, self.output.width))\n",
        "        return ret\n",
        "\n",
        "    def _convert_keypoints(self, keypoints):\n",
        "        if isinstance(keypoints, Keypoints):\n",
        "            keypoints = keypoints.tensor\n",
        "        keypoints = np.asarray(keypoints)\n",
        "        return keypoints\n",
        "\n",
        "    def get_output(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            output (VisImage): the image output containing the visualizations added\n",
        "            to the image.\n",
        "        \"\"\"\n",
        "        return self.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYomOGmdbAIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo",
        "colab_type": "text"
      },
      "source": [
        "# Train on a custom COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_",
        "colab_type": "text"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the fruits nuts segmentation dataset](https://github.com/Tony607/mmdetection_instance_segmentation_demo)\n",
        "which only has 3 classes: data, fig, and hazelnut.\n",
        "We'll train a segmentation model from an existing model pre-trained on the COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that the COCO dataset does not have the \"data\", \"fig\" and \"hazelnut\" categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RhkndJ6JWqO",
        "colab_type": "code",
        "outputId": "8281696b-2c24-4405-8c82-f3805b1b1c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW",
        "colab_type": "text"
      },
      "source": [
        "Register the fruits_nuts dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-aG4sj3cV2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "下面 笔画数据集\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Retbdmc07rgd",
        "colab_type": "code",
        "outputId": "4af19142-f2fa-40f5-b9ba-b1b8c8b77e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wz\", {}, \"./drive/My Drive/pic32/train.json\", \"./drive/My Drive/pic32/img\")\n",
        "wanzheng_metadata = MetadataCatalog.get(\"wz\")\n",
        "wanzhengdataset_dicts = DatasetCatalog.get(\"wz\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 02:16:25 d2.data.datasets.coco]: \u001b[0mLoaded 680 images in COCO format from ./drive/My Drive/pic32/train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9EZCsT__S3i",
        "colab_type": "code",
        "outputId": "2c5c50c4-e9d2-41f7-c3d5-7eabb369a584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wz1100\", {}, \"./drive/My Drive/pic32_907A/trainA.json\", \"./drive/My Drive/pic32_907A/img\")\n",
        "wanzheng1100_metadata = MetadataCatalog.get(\"wz1100\")\n",
        "wanzheng1100dataset_dicts = DatasetCatalog.get(\"wz1100\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 02:16:26 d2.data.datasets.coco]: \u001b[0mLoaded 907 images in COCO format from ./drive/My Drive/pic32_907A/trainA.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E",
        "colab_type": "text"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5JPh6Ur8FTD",
        "colab_type": "code",
        "outputId": "a32b4f33-a043-4cb4-e388-09dbe6d6608c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "#笔画数据集\n",
        "import random\n",
        "from detectron2.utils.colormap import random_color\n",
        "\n",
        "for d in random.sample(wanzheng1100dataset_dicts, 1):\n",
        "\n",
        "    print(d)\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wanzheng_metadata, scale=0.5)\n",
        "    # vis = visualizer.draw_dataset_dict(d)\n",
        "    #去掉类别的显示\n",
        "    vis = visualizer.draw_dataset_dict_no_cls(d)\n",
        "    \n",
        "    cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "    \n",
        "\n",
        "    # visualizer1 = Visualizer(img[:, :, ::-1], metadata=wanzheng_metadata, scale=0.5)\n",
        "    # annos = d.get(\"annotations\", None)\n",
        "    # if annos:\n",
        "    #         if \"segmentation\" in annos[0]:\n",
        "    #             masks = [x[\"segmentation\"] for x in annos]\n",
        "    #         else:\n",
        "    #             masks = None\n",
        "    # masks = visualizer1._convert_masks(masks) # _convert_masks:Convert different format of masks or polygons to a tuple of masks and polygons.\n",
        "    # print(masks) # GenericMask:\n",
        "    # num_instances = len(masks)\n",
        "    # print(num_instances)\n",
        "\n",
        "\n",
        "    # areas = np.asarray([x.area() for x in masks])\n",
        "    # sorted_idxs = np.argsort(-areas).tolist()\n",
        "    # masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
        "    \n",
        "    # assigned_colors=None\n",
        "    # if assigned_colors is None:\n",
        "    #  assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "\n",
        "    # for i in range(num_instances):\n",
        "\n",
        "    #   color = assigned_colors[i]\n",
        "    #   for segment in masks[i].polygons: \n",
        "    #    print(segment.reshape(-1, 2))#获得每个分割图的坐标矩阵 polygons\n",
        "\n",
        "    #    vis1 = visualizer1.draw_polygon(segment.reshape(-1, 2), color, alpha=0.5)\n",
        "    #    cv2_imshow(vis1.get_image()) \n",
        "\n",
        "\n",
        "    # #   #  已知边界数组polygons 如何画出灰度图 python\n",
        "\n",
        "      "
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'file_name': './drive/My Drive/pic32_907A/img/000396.jpg', 'height': 256, 'width': 256, 'image_id': 431, 'annotations': [{'iscrowd': 0, 'bbox': [44.0, 34.0, 63.0, 61.0], 'category_id': 3, 'segmentation': [[85.81706063720452, 34.840698869475844, 85.71428571428571, 44.39876670092497, 83.65878725590956, 51.3874614594039, 77.28674203494347, 62.38437821171634, 67.0092497430627, 74.40904419321686, 62.898252826310376, 79.85611510791367, 44.91264131551901, 94.96402877697841, 48.61253854059609, 95.47790339157245, 64.74820143884892, 86.8448098663926, 82.73381294964028, 72.4563206577595, 102.05549845837615, 55.08735868448098, 107.09146968139774, 50.15416238437821, 107.60534429599177, 47.27646454265159, 101.02774922918807, 40.8016443987667, 89.92805755395683, 34.73792394655704]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [20.0, 84.0, 82.0, 91.0], 'category_id': 3, 'segmentation': [[81.5005138746146, 84.89208633093524, 79.65056526207604, 86.22816032887975, 80.16443987667009, 91.26413155190133, 81.29496402877697, 94.14182939362794, 81.08941418293936, 99.28057553956835, 79.44501541623843, 105.44707091469681, 75.53956834532374, 114.4912641315519, 64.74820143884892, 129.39362795477902, 51.69578622816033, 146.76258992805754, 37.30729701952723, 161.25385405960944, 20.452209660842755, 175.64234326824254, 24.563206577595064, 175.84789311408016, 36.07399794450154, 171.2230215827338, 58.06783144912641, 153.75128468653648, 72.3535457348407, 138.64337101747174, 86.02261048304213, 119.7327852004111, 102.67214799588899, 99.69167523124358, 102.67214799588899, 96.91675231243576]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [150.0, 98.0, 19.0, 84.0], 'category_id': 9, 'segmentation': [[150.7246376811594, 100.89186176142697, 153.73467112597547, 110.81382385730211, 155.51839464882943, 182.16276477146042, 167.3355629877369, 181.60535117056855, 167.22408026755852, 117.94871794871794, 169.11928651059085, 110.36789297658862, 169.3422519509476, 104.34782608695652, 163.99108138238572, 100.55741360089185, 162.20735785953175, 98.66220735785953]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [67.0, 133.0, 18.0, 91.0], 'category_id': 9, 'segmentation': [[72.4, 140.1, 74.7, 149.3, 74.6, 170.1, 72.5, 186.2, 68.0, 202.0, 67.9, 209.6, 74.0, 222.1, 77.8, 224.5, 81.4, 220.4, 84.3, 192.1, 85.4, 173.1, 85.8, 150.7, 85.3, 143.6, 80.7, 138.0, 77.0, 133.6]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [124.0, 76.0, 81.0, 24.0], 'category_id': 1, 'segmentation': [[124.41364605543708, 96.80170575692964, 132.30277185501066, 99.99999999999999, 150.10660980810232, 100.63965884861406, 186.1407249466951, 92.9637526652452, 202.45202558635393, 87.1002132196162, 205.863539445629, 84.22174840085287, 201.81236673773986, 79.8507462686567, 191.5778251599147, 76.65245202558634, 125.58635394456289, 94.77611940298506]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [120.0, 126.0, 79.0, 22.0], 'category_id': 1, 'segmentation': [[120.14925373134328, 141.68443496801706, 130.49040511727077, 147.76119402985074, 136.6737739872068, 148.4008528784648, 143.07036247334753, 148.50746268656715, 155.01066098081023, 147.6545842217484, 198.4008528784648, 136.78038379530915, 199.89339019189765, 134.22174840085287, 197.86780383795306, 130.49040511727077, 188.0597014925373, 126.97228144989337, 175.1599147121535, 131.2366737739872, 141.04477611940297, 137.73987206823026]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [90.0, 175.0, 148.0, 26.0], 'category_id': 1, 'segmentation': [[90.6, 192.4, 92.6, 195.5, 103.9, 201.7, 113.9, 201.4, 154.0, 194.1, 179.3, 191.3, 237.6, 191.2, 238.8, 189.1, 238.8, 186.0, 231.0, 178.2, 220.9, 175.2, 168.6, 181.7, 151.3, 183.5, 108.9, 190.3, 92.7, 190.2]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [141.0, 37.0, 39.0, 26.0], 'category_id': 17, 'segmentation': [[141.9, 37.9, 142.2, 39.9, 155.5, 53.8, 171.8, 63.7, 176.7, 63.9, 180.2, 58.4, 180.6, 52.4, 178.5, 47.5, 172.3, 43.2, 161.6, 39.8, 150.7, 38.0]], 'bbox_mode': <BoxMode.XYWH_ABS: 1>}]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7FDC6765A9E8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA",
        "colab_type": "text"
      },
      "source": [
        "Now, let's fine-tune a coco-pretrained R50-FPN Mask R-CNN model on the fruits_nuts dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqKpPAmApFOH",
        "colab_type": "text"
      },
      "source": [
        "## MaskRCNN模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STxs_A9DJjCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.data.datasets import register_coco_instances\n",
        "# register_coco_instances(\"wzMRCNN\", {}, \"./drive/My Drive/pic566_28class/images566.json\", \"./drive/My Drive/pic566_28class/images\")\n",
        "# wanzheng_metadata = MetadataCatalog.get(\"wzMRCNN\")\n",
        "# wanzhengdataset_dicts = DatasetCatalog.get(\"wzMRCNN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 创建MaskRCNN模型\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg1 = get_cfg()\n",
        "cfg1.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg1.DATASETS.TRAIN = (\"wz1100\",)\n",
        "cfg1.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg1.DATALOADER.NUM_WORKERS = 4\n",
        "# cfg1.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "cfg1.MODEL.WEIGHTS =\"/content/drive/My Drive/05171628-MR10000/model_final.pth\"\n",
        "cfg1.SOLVER.IMS_PER_BATCH = 10\n",
        "cfg1.SOLVER.BASE_LR = 0.02\n",
        "cfg1.SOLVER.MAX_ITER = 5000   # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg1.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "cfg1.MODEL.ROI_HEADS.NUM_CLASSES = 32 # 32 classes (data, fig, hazelnut)\n",
        "cfg1.OUTPUT_DIR='/content/drive/My Drive/MR15000'\n",
        "os.makedirs(cfg1.OUTPUT_DIR, exist_ok=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PrnUOVOy_xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "800558df-3909-4086-d65d-ca4f18da6600"
      },
      "source": [
        "MRCNNtrainer = DefaultTrainer(cfg1)\n",
        "MRCNNtrainer.resume_or_load(resume=False)\n",
        "MRCNNtrainer.train()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 02:44:19 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=33, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=128, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (predictor): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[05/24 02:44:19 d2.data.datasets.coco]: \u001b[0mLoaded 907 images in COCO format from ./drive/My Drive/pic32_907A/trainA.json\n",
            "\u001b[32m[05/24 02:44:19 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 907 images left.\n",
            "\u001b[32m[05/24 02:44:19 d2.data.build]: \u001b[0mDistribution of instances among all 32 categories:\n",
            "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
            "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
            "|    piezhe     | 107          |     heng      | 1375         | hengzhewangou | 23           |\n",
            "|      pie      | 984          |      na       | 231          |   shuwangou   | 85           |\n",
            "|    henggou    | 63           |    shugou     | 120          |   shuzhezhe   | 11           |\n",
            "|      shu      | 821          |    hengzhe    | 241          |  hengzhegou   | 176          |\n",
            "| hengzhezhezhe | 12           | hengzhezhez.. | 12           |    hengpie    | 81           |\n",
            "|  hengzhezhe   | 11           | shuzhezhegou  | 34           |     dian      | 520          |\n",
            "|    wangou     | 38           |      ti       | 170          |     shuti     | 29           |\n",
            "|    shuzhe     | 48           |     wogou     | 26           |    xiegou     | 30           |\n",
            "| hengzhezhepie | 29           |  hengzhewan   | 23           |    piedian    | 37           |\n",
            "|   shuzhepie   | 12           |  hengxiegou   | 11           |   hengzheti   | 25           |\n",
            "|    shuwan     | 21           | hengpiewangou | 22           |               |              |\n",
            "|     total     | 5428         |               |              |               |              |\u001b[0m\n",
            "\u001b[32m[05/24 02:44:19 d2.data.common]: \u001b[0mSerializing 907 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[05/24 02:44:19 d2.data.common]: \u001b[0mSerialized dataset takes 2.25 MiB\n",
            "\u001b[32m[05/24 02:44:19 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[05/24 02:44:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[05/24 02:44:21 d2.engine.train_loop]: \u001b[0mStarting training from iteration 10000\n",
            "\u001b[32m[05/24 02:44:21 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:00 (0:00:00 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks5OWDru0u1G",
        "colab_type": "text"
      },
      "source": [
        "## 如何不显示cls??  I did!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4mZwgPUoXv1",
        "colab_type": "code",
        "outputId": "7f3d00ae-aa7c-4c39-ed91-c9bf791cf064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## 创建MaskRCNN模型\n",
        "# cfg1.MODEL.WEIGHTS = os.path.join(cfg1.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "cfg1.DATASETS.TEST = (\"wz\", )\n",
        "MRCNNpredictor = DefaultPredictor(cfg1)\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):    \n",
        "    # im = cv2.imread(d[\"file_name\"])\n",
        "    im = cv2.imread('/content/drive/My Drive/pic566_28class/images/000396.jpg')\n",
        "    outputs = MRCNNpredictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=wanzheng_metadata, \n",
        "                   scale=0.8, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels用于实例化可视化的不同颜色模式  IMAGE_BW：与IMAGE相同，但将所有不带遮罩的区域转换为灰度。仅适用于按实例绘制蒙版预测\n",
        "    )\n",
        "\n",
        "    #重写的 函数 draw_instance_predictions_no_cls\n",
        "    v = v.draw_instance_predictions_no_cls(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "\n",
        "    #如何输出单独的mask\n",
        "    masks=np.asarray(outputs[\"instances\"].to(\"cpu\").pred_masks)#[n,256,256]n为笔画数\n",
        "\n",
        "    for mask in masks:\n",
        "      mask=mask.astype(\"uint8\")#mask从[false,flase]到[00001100]\n",
        "      mask=mask*255       #变成二值图\n",
        "      cv2_imshow(mask)\n",
        "\n"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=204x204 at 0x7FDBD7AD4780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABm0lEQVR4nO3b0U7DQAwEwID4/18ODwgoJQh6sS/2MfPe1OvbpFSi2wYAAAAAAAAAAEP2qwf4o+ek6+5dNvCUctX38DlXD5XVgDcNWpDbgMS3iJLbgAYmLGAvfSNMaUDlDcy5BQqXYNYzoOwKPASnvVPRDmjA1QNczQKuHuBq/34BGd9Ufnjc1/xSNK8BNfNnLOC4AEXzT2tA1fyzFlA2f/xkRzdA3fg+BsMP56AAlc8/vAHt8m8vydcvHj/9GVA+f/AC7u+A+vljF9Awf+gCOuaPHPIuf4v4/hAKPKevBWhy/hoQdlJNzz+pAY3yB83a9vxTGtAqf8wCGv1HzDcRC+icP2IBrfMHLKB3/vMLaJ7/7Mjd44d+DLbMf27q2/PvGT+uAV3znxl8hfOPakDf/OOj35x/4/jjDVgl/+j0n/l7xx+d/yN/9/iDt8BC+YciNPpJ1O8GGrBU/oEU++gLa3q4AYvlfzjIPvKiykY+BVbKDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJDvFTLYLWewaGo7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD47B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACFklEQVR4nO3cy1LDMAwFUMP//3PZwFAeKXEiFUs5Z8EGJs29lh1KZxgDAAAAAAAAAAAAAAAAAAAAAAAAAACAOl7Cr3jLuWyW15zL3nIumyCpgDoNJG2BpGsnyJqAMYpMQWYBJRrI3AJJLxArdQLGWH8K0gtYvYH0LZD0KmHyJ2CMpafgOQUsTAH/fQP/TQFPeZWrPwUWzh9fwC9PvJXzhxdQLf8TtsDa+aML+DkAi+f3GMwuYPUBMAHJBSw/ALF3+OMIXD+/LRBZQMUBSJ2ACvkDb7Lk+jsDEguoMQBxBXzfAUXyp01AlfxhBRT7RPBTUAFl8+dsgUL5gwpY+KOvv4QUUHcDxBRQOX9EAaXzxx+CxfKfv9/a6+/N0OkCqg/A2QIK/wLw7lwBVd8C3gk9AwrmdwieKqD8CThMwJlV67D+JuBEAT0G4HgBTfIfLqBL/qAzoG7+owXUfw/w4dDatZn/4TF4qIBOA3CkgFb5DxTQK/98Ac3yTxfQLf9sAe3yn3sMNsg/meHLAHSIP5eiYfzjW6BL/pkC7gegTf6JAnrm319A0/y7C+iaf28BbfPvLKBv/vnHYLP8u/I0Xv9dge7yt4u/Zwv0zv93pmL/G23a41TNV38MfxZ/XMAFBuBRsNuOn6lvewKukX+7gIvk34x3e/ztPjYm4DL5twp4ufsKAAAAAAAAAAAAAAAAAAAAAAAAALCWN32UPKsM9OLtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD4BE0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAA6klEQVR4nO3VSQ7CQAwEwEbi/1+GExyQWDQL9khVp+Tm7nEyCQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQ7VI9wKDb82kywZEF3F7eZ0JcZwap8Bp+1lEb8CH8cI5jNmD1yT+cUMCu7EnaF7A1e5LWBewPn3Qt4D/Zk7S7BcaSz4ToU8Dgqc8GaFHA8MYvmL60gKlPfdHkdQV0SJ+ut8BHa8+s8hMY2IH14x70D9gzavEt8GsF+8asvwa/dLB7wPoC8qaDFpMBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABV7jXzEyWBSgh6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD47B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABGElEQVR4nO3Xy27CMBAF0EHq//8yLKjaAnHSloxnEp2zi1jY98YPEgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7O1SPYEE18fH9YjnKeA6/mkt5AkKWEn+ZRzzwAX8JvgPg6Qf709ksj/m3nKUAnaO/a37Ftgx+HLUpgUkvfCFtO0KSFvrEbEUt1EBudE/veRtcQhOiT5QXUBl9oioLKA8+l3FGVAY/TXuvBXQ5I0/y1wBh/j3lljAvvmzJlp9C2xJP6NSB3hnDcw6nbPH+UcHcy+mVlug4k5uUkDdJ0n+yBuboPprbMr4gw6qs0fErEk8N9AiOgAAAAAAAAAAAAAAAACczg1YDBYyMcqUnAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD4828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAA7UlEQVR4nO3YwQ4BURBE0eb//5kNiQhvIdOtHuesrKTc9MxCFQAAAAAAAAAAAL/r9O0BTy73D1PDYgPctO9LD1DVvDEtwMsCVX1D4wK8LdC0NS/AokDH2sAAqwLH7z0f/YUHWP3IVZyPJAYYLRAZYLJAZoDBAqEB5l7OqQHGCsQGmCqQG6BOIwmCA8wcQXSAiQLZAQYeg/AA/UcQH6C7QH6A5gIbBOh9EewQoPUI9gjQeASbBHg8gsQ/sQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgL92BUpzCz+qSohpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD46A0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABbElEQVR4nO3csW4CQRBEwcX//8/nDEPiaOAxoipzsupr9a2tC3wOAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZG6DZ13D573Fz9xR19xRbzRYwDlnYQtzk70/+q63YHoB6yhg/shr1T1gAXWA2lwBD5f/pnfAAuoANQXUAWqDBey8BS2gDlBTQB2g9poCFn0SsIA6QE0BdYCaAuoANQXUAWoKqAPUFFAHqCmgDlBTQB2gpoC5oxZ9C39gAXWAmgLqALWXFLDoq/hgATt/CXgFFFAHqCmgDlBTwNRBS38LWoAC6gC1qQK2XgEWoIA6QG2ogLVXgAUooA5Qmyng+QrY9EnQAhRQB6gpoA5QGylg79+BFqCAkQJu//z06SygDlBTQB2gpoCRU3Zd/E8soA5QGypg7ztgAeMnLhvDVAHLHvvP2AK2NvD1d8D0f5Vdt4TpBWx7/tEC1j08AAAAAAAAAPAdfgGHDxCx987iEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD4B70>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABAklEQVR4nO3X0QqCQBAF0An6/1+2lyICMXd21R0556kCl7nXdaUIAAAAAAAAAAAAAAAAAAAAAAAAgIIeVw/QYfl86AlRtoDl51s+RskClpXfskGqFbCW/S0XpVIBG+EjIpmlTAH/0kfkwtQoYE/6iFSaZ/slJ9sbPmnyHdCW/maPQPOtv9UhmNj493kNZp76dI7pCjg1fee1w6UO/M4E0xSQe9v1jz9HAVelH7VIlwvDD10o6ewz79jFmrXHHz7v/P8Fvg65WUUKOG6jFjgDjh3x6gJiq4MJhgMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAZ5AZu0FSL3iUTvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD47B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABLUlEQVR4nO3awQqCQABFUev//9lWQQSKi2buSOesbBOPyxiCbRsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAs7VEPOLG/L0aOXDbA/vlh4MrnuK++h3ucgG3c0L8/AQLUA676viV+ZdkAs36clg0wiwD1gJoA9YCrPAgNIkA94NCkB4F1A0xylwDDzsNdAgwjQD2gJkA9oCZAPaAmQD2gJkA9oCZAPaAmQD2gJkA9oCZAPaAmQD2gJkA9oCZAPeDYnHdjCweYQ4B6QE2AekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6wIkp/5NbOQAAAAAAAAAAAAAAAAAAAAAAAACceAEzTwSpDaaG8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDBD7AD46A0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaXRLPfqtJjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputMRCNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyUCDYnqHOKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wz\", cfg1, False, output_dir=\"./outputMRCNN/\")#output_dir可选的输出目录，用于转储数据集上预测的所有结果。转储包含两个文件 \"instance_predictions.pth\" “ coco_instances_results.json”\n",
        "val_loader = build_detection_test_loader(cfg1, \"wz\")\n",
        "inference_on_dataset(MRCNNtrainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdCjVuZ7gSxQ",
        "colab_type": "code",
        "outputId": "51bb43b5-bac6-4635-ee40-7d5f93bc84e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    outputs = MRCNNpredictor(im)\n",
        "    delta = time.time() - start_time\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1 / mean_delta\n",
        "print(\"Average(sec):{:.2f},fps:{:.2f}\".format(mean_delta, fps))"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average(sec):0.07,fps:14.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSV_XTI1tJTw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9_YjV7o7xk",
        "colab_type": "text"
      },
      "source": [
        "## Pointrend模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuB2wY_8kCv",
        "colab_type": "code",
        "outputId": "83cf098e-c00a-4fbf-97f5-949b871a9cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Add PointRend-specific config\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 32#修改POINT_HEAD.NUM_CLASSES 32 默认值为80\n",
        "\n",
        "cfg.merge_from_file(\"/content/detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "# cfg.merge_from_file(\"https://github.com/facebookresearch/detectron2/blob/master/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"wz1100\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "# cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "cfg.MODEL.WEIGHTS ='/content/drive/My Drive/pp25000/model_final.pth'\n",
        "cfg.SOLVER.IMS_PER_BATCH = 10\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 25000    # 3000 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE =256   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 32  # 28 classes (heng,shu....)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 32#修改POINT_HEAD.NUM_CLASSES 32 默认值为80\n",
        "cfg.OUTPUT_DIR='/content/drive/My Drive/pp25000'\n",
        "#正式训练\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 02:48:15 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): PointRendROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=33, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=128, bias=True)\n",
            "    )\n",
            "    (mask_coarse_head): CoarseMaskHead(\n",
            "      (reduce_spatial_dim_conv): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (coarse_mask_fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (coarse_mask_fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (prediction): Linear(in_features=1024, out_features=1568, bias=True)\n",
            "    )\n",
            "    (mask_point_head): StandardPointHead(\n",
            "      (fc1): Conv1d(288, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc2): Conv1d(288, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc3): Conv1d(288, 256, kernel_size=(1,), stride=(1,))\n",
            "      (predictor): Conv1d(288, 32, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[05/24 02:48:15 d2.data.datasets.coco]: \u001b[0mLoaded 907 images in COCO format from ./drive/My Drive/pic32_907A/trainA.json\n",
            "\u001b[32m[05/24 02:48:15 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 907 images left.\n",
            "\u001b[32m[05/24 02:48:15 d2.data.common]: \u001b[0mSerializing 907 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[05/24 02:48:15 d2.data.common]: \u001b[0mSerialized dataset takes 2.25 MiB\n",
            "\u001b[32m[05/24 02:48:15 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[05/24 02:48:15 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[05/24 02:48:21 d2.engine.train_loop]: \u001b[0mStarting training from iteration 25000\n",
            "\u001b[32m[05/24 02:48:21 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:00 (0:00:00 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJE2meNB3wNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IoNrGiVj3wkB",
        "colab": {}
      },
      "source": [
        "#PointRend输出高分辨率分割图\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Add PointRend-specific config\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 32#修改POINT_HEAD.NUM_CLASSES 32 默认值为80\n",
        "\n",
        "cfg.merge_from_file(\"/content/detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "# cfg.merge_from_file(\"https://github.com/facebookresearch/detectron2/blob/master/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"wz1100\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "# cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "cfg.MODEL.WEIGHTS ='/content/drive/My Drive/05171809-pp15000/model_final.pth'\n",
        "cfg.SOLVER.IMS_PER_BATCH = 10\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 25000    # 3000 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE =256   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 32  # 28 classes (heng,shu....)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 32#修改POINT_HEAD.NUM_CLASSES 32 默认值为80\n",
        "cfg.OUTPUT_DIR='/content/drive/My Drive/pp2500'\n",
        "#正式训练\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-HbfLDNHojH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF",
        "colab_type": "text"
      },
      "source": [
        "Now, we perform inference with the trained model on the fruits_nuts dataset. First, let's create a predictor using the model we just trained:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6RCjvB9vU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "# %tensorboard --logdir  output\n",
        "%tensorboard --logdir \"/content/drive/My Drive/05171809-pp15000\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCp8IQ-aF53h",
        "colab_type": "text"
      },
      "source": [
        "## inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjBt4H072_O",
        "colab_type": "code",
        "outputId": "9857389a-287b-4dc1-9aea-b67da4490476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#注册inference的数据集\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wzInfer\", {}, \"./drive/My Drive/pic566_28class/images566.json\", \"./drive/My Drive/pic566_28class/images\")\n",
        "wanzheng_metadata = MetadataCatalog.get(\"wzInfer\")\n",
        "wanzhengdataset_dicts = DatasetCatalog.get(\"wzInfer\")"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 02:48:54 d2.data.datasets.coco]: \u001b[0mLoaded 566 images in COCO format from ./drive/My Drive/pic566_28class/images566.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.WEIGHTS =\"/content/drive/My Drive/pp25000/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "cfg.DATASETS.TEST = (\"wzInfer\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO",
        "colab_type": "text"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM",
        "colab_type": "code",
        "outputId": "ee46133b-ae5d-419c-f092-96e2d7d8fdc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "import random\n",
        "\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):    \n",
        "#     im = cv2.imread(d[\"file_name\"])\n",
        "    im = cv2.imread('/content/drive/My Drive/pic566_28class/images/000396.jpg') #使用活字 的图片\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=wanzheng_metadata, \n",
        "                   scale=0.8, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels用于实例化可视化的不同颜色模式  IMAGE_BW：与IMAGE相同，但将所有不带遮罩的区域转换为灰度。仅适用于按实例绘制蒙版预测\n",
        "    )\n",
        "    # v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    v = v.draw_instance_predictions_no_cls(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "\n",
        "    #如何输出单独的mask\n",
        "    masks=np.asarray(outputs[\"instances\"].to(\"cpu\").pred_masks)#[n,256,256]n为笔画数\n",
        "    count=0\n",
        "    for mask in masks:\n",
        "      mask=mask.astype(\"uint8\")#mask从[false,flase]到[00001100]\n",
        "      mask=mask*255       #变成二值图\n",
        "      cv2_imshow(mask)\n",
        "\n",
        "      count=count+1\n",
        "      file_path=\"/content/ppqiu/\"+str(count)+\".jpg\"\n",
        "      cv2.imwrite(file_path,mask)\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=204x204 at 0x7FDC68764F60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACMklEQVR4nO3ay24bMQwFULX//8/TRQo0TmPPQ+REpM/ZxhB8rymOEXgMAAAAAAAAAAAAAAAAAAAAAAAAAAAA6vgVfeCWcWii3xmHblvGqTlSCqhUQVIBdSpIK+BjG6wvsYAaQxC/sB9TL/9AyJyAMQpMQXYBy1eQX8Di2/COApYeglsKWJkCfvoN/DQFRB/47b5b+OvQLROwcH5X4I4CVh6A8AK+WQFL579hAtbOn1/A4vmjC/jvBqye31MgtoB6A5A8Aevnjy3g6wAUyG8HRBZQcQAyJ6BE/sgCvgxAjfyBBdTMH1fAwv/4fSlrB1QZgLACil6AsALK5s+5AoXyBxVQdQOOoALqXoCUK1Aqf0gBxX4T8iiggNL5AwoovADHiN8B1QZgvoDaF2C+gOIXIPoK1BuA2QLKD8BkAZW/Av419Z6rL8Ax5iagQ/6ZAurf/zECnwJFB2CigBYXYKKAHhfgegENHoAfYnZA3fxXC2iyAMbVAvrkv1ZAo/wBO6B2/ksFPAxA8fxXCmiV/0IBvfKfTtBp/40xJpdgg/xnC2g2/+NsAf3ynyugYf5TBXTMf6aAlvlPBPmcv038ExPQNP/hArrmP1pA2/wHC+ib/1gBjfMfyvMpf7f4hyagdf4DBfTOv19A8/y7of7lbxl/L1b3j3/s5Gr/8Y/XO+Ad8r+Kth14TX3Pw217L+jh6RV4k/xPA24v//oOuvwEBgAAAAAAAAAAAAAAAAAAAAAAAADo5w9cNUue0OmnygAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC687645F8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABsklEQVR4nO3by07EMBAEQIP4/18OJ8Q+glCcGWvGqron6+70RsuBMQAAAAAAAAAAYGsfObc98m4d7DPv1seRd+84iQvIvH+cxAWMMRrMILuAhzHUlF9A8REsKKD2CJYUUHkEawooPIJVBZRtYFkBVRtYV0DRBnIKOM9asoGFC6gppYCSj/oPKxdQ8g+jhQWUzJ9SwPk3oGb+dQsomj+jgNMBVM2fUECv/ONrxYfUjZ+wgJMBVM6/4iVYOn94Ae8DqJ0/uoB2+dO/AtXzBxfwNoDy+WNP+Jq/fvzYBXTMn/kOaJE/soCXAfTIH1hAz/xx53zO3yV+1jugT/6wAp4G0Ch/VAFt8wcV0Dd/TAGN84cc9zF/s/ghC2idP6CA3vnvF9A8f+gPoY75bxfwMICW+W+euvv+x81jt3/849bBN3j84847YI/880ffYf5jzC9gl/yzp//N3zv+7AL2yT8XoM8/xPxvZgE75Z/J8JN/h/gzKY7ZC2u6nOOYu6ysq0mOmYsqu/gS3C4/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQL5vHp83ZII0M5gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC687649B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABFklEQVR4nO3YwQ6CMBRFQU38/1/WhSIaW0Hs6ytkZmdclHMTA3g6AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACtnbMvoLnr+8elwCMNcK198S3yCANUw2f1zD0PsCJ8Vgvd4wA/hT9VSnc1wLbySTl1FwP8F/5UbB13gEbVr0qxIw4QkP5QqB1rgLj0h8/cS/SR64SXV6UPkJd+lzhAdvpdwgBjhE+6DTBW9iz0LhAT3faSIwdo2x90pel3gSXRDyrD/gR6PaHFnrNpgb4Pp8Gn/bZAxnN5+JnrJsh7JYk/eWGB7LexHudv+re2ly7XUFhghHYAAAAAAAAAAAAAAAAAOJobeEcYKDbX2LkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC68764FD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABSUlEQVR4nO3cwWrCQBhG0dj3f+e4bRdCqf3nzuA5K1fJ50VkUMh1AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwDvue/gGj+Hr/923dz458mvw2m+5X7z+b9sG+GGwwBkBBgscEmCuwCkBxgocE2CKAPWA2rYBVp3Qtg2wigD1gNoxAaa+E44JMEWAekBNgHrAL42di04JMGbfAIvOwvsGWESAekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6QE2AekBNgHpATYB6wGtr/hnZOMAaAtQDagLUA2oC1ANqAtQDagLUA2oC1ANqAtQDagLUA2o7B1jyk9DOAaafnXBd194BPv4TsIQA9YCaAPWAmgD1gJoA9YDazgEchVfcZOcASwhQD6gJUA+oHRJg3wdfAgAAAAAAAAAAAAAAAAAAAAAAUHkC8kELpeafSowAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC687649B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABcElEQVR4nO3bQU4DQRADwCz///NyiqIgTsiD5ajqAqeWY3ombKQ8HgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHyIux3gD76Cs+7FBpIFPBZ34IpNer723MR/Ed6APfkCxk7BgQ3YauDEEZhqwB1wYujSCuQKGHv7e3IE2gHaFNAO0KaA3KjNtwEb0A7QpoDcqKV/gF9sQDtAmwLaAdoU0A7QpoB2gDYF5EZ5GpzkWaAdoE0B7QBtCmgHaFNAO0CbAtoB2hTQDtCWK2DzUcAGKKAdoE0B7QBtsQJG3wRsgAJODF36gNwGhOas3oE2QAHtAG2hAmavABuggHaAtkwBu1eADVBAO0DbiQKWHgYzBQzfgY6AAtoB2iIFTN16P9iAdoC2AwVsHQgb0A7Qlilga+vf2IB2gDYFZMZcv/66wAaE5oz93V9iG7DagCMQm3S9/ZiR/ubo2uvPupc/HQUAAAAAAAAAPtY3mxUTrsYVx1MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC68764FD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAA8ElEQVR4nO3VwQrCMBAE0BX8/1+uhx6EYqluEjcp7530IMxMaxIBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFR7VAdI2t4f2yosOcB2+N5SYr0Bju0jmlqsNcCn8rt0j3UGOC8fEfkiz+Tv/uuifIvZBxhYfTfzAMPLR0x6BiSa3+UQTD70W1yD2Re+scEMA+T/6x3S1w7QdMz1iV44QEv7frHrBkj37xt5sTegf9zSM+CnCQYlLb4FvppgaMbya/BqgtEByweIOB9hinAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAECJF3zPFBuOGTcMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC68764588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABBUlEQVR4nO3WQQ6CMBAF0DHx/lfGhRtNCNBpS1vy3sZIYjP/l4IRAAAAAAAAAAAAAAAAAAAAAAAAAAt6jR6gwvb9qIuwagHb/9d8jBUL2PYuZoOsVsBu+K9clKUKOEgfkcyyTgEn6SNyYdYo4EL4iEileZf/5G5X0+fMXkDf9DH3ESgO/6RnQGLnn/MazNz26RyzFZA78xUppioglb4ywTQF3L71zVZoYVT6VotUSb7pWw0+uICBW99hrXLl+ZvPO/tf4V9dNmuRAvrdqKMfgmeHoPt8owuIgw4mmA0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGjlA/LpFSTonmUrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC68764FD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAABAklEQVR4nO3Yuw7CQAxFwYX//+dQIEC8UqC1uQszVarIOfJGSsYAAAAAAAAAAADgdx2+PcC97XbZNFlugLPy+cICPBcYo3bItACvC4y6QeMCvC1QNOux4qZFtvdtPpe3ATsrMArmDQywn2D2wJFHYO8hZx+DyACdBTIDNBYIDdD3bkoNsFdg6grEBujagdwATQWCA4xDR4LkAC1LkB2goUB4gPoC6QHKXwTxAaqXYIEAtUuwQoDHJZjaY40Ahccg8ofIS9cvgLkjrxPgkmCliQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4E+cAO+gDzrdm824AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7FDC687649B0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bo0cypwllj",
        "colab_type": "code",
        "outputId": "fb2b3f28-2dd3-4c33-a2a0-444a6580460f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "wanzheng_metadata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metadata(evaluator_type='coco', image_root='./drive/My Drive/pic566_28class/images', json_file='./drive/My Drive/pic566_28class/images566.json', name='wzInfer', thing_classes=['piezhe', 'heng', 'hengzhewangou', 'pie', 'na', 'shuwangou', 'henggou', 'shugou', 'hengzhegou', 'hengzhezhezhegou', 'hengpie', 'shu', 'shuzhezhegou', 'dian', 'wangou', 'ti', 'shuti', 'shuzhe', 'wogou', 'hengzhe', 'xiegou', 'hengzhezhepie', 'hengzhewan', 'piedian', 'shuzhepie', 'hengxiegou', 'hengzheti', 'shuwan'], thing_dataset_id_to_contiguous_id={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvn2tueICLiE",
        "colab_type": "text"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API. This gives an AP of ~70%. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGKtdLlNFZk2",
        "colab_type": "code",
        "outputId": "cfd9b7bc-7258-4705-8551-972c65738b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(cfg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('wzInfer',)\n",
            "  TRAIN: ('wz1100',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: bitmask\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32], [64], [128], [256], [512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_resnet_fpn_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: True\n",
            "  META_ARCHITECTURE: GeneralizedRCNN\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  POINT_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    COARSE_PRED_EACH_LAYER: True\n",
            "    FC_DIM: 256\n",
            "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
            "    IN_FEATURES: ('p2',)\n",
            "    NAME: StandardPointHead\n",
            "    NUM_CLASSES: 32\n",
            "    NUM_FC: 3\n",
            "    OVERSAMPLE_RATIO: 3\n",
            "    SUBDIVISION_NUM_POINTS: 784\n",
            "    SUBDIVISION_STEPS: 5\n",
            "    TRAIN_NUM_POINTS: 196\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: FastRCNNConvFCHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 2\n",
            "    POOLER_RESOLUTION: 7\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: True\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: PointRendROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 32\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.5\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    IN_FEATURES: ('p2',)\n",
            "    NAME: CoarseMaskHead\n",
            "    NORM: \n",
            "    NUM_CONV: 4\n",
            "    NUM_FC: 2\n",
            "    OUTPUT_SIDE_RESOLUTION: 7\n",
            "    POINT_HEAD_ON: True\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 1000\n",
            "    PRE_NMS_TOPK_TEST: 1000\n",
            "    PRE_NMS_TOPK_TRAIN: 2000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/drive/My Drive/05171809-pp15000/model_final.pth\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  BASE_LR: 0.00025\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 10\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 12000\n",
            "  MOMENTUM: 0.9\n",
            "  STEPS: (210000, 250000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWn8_HF-GBD3",
        "colab_type": "text"
      },
      "source": [
        "## **检验效果---AP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Y_TQ6YCOWT",
        "colab_type": "code",
        "outputId": "1e99c097-f5e2-4b30-d6d7-53d6455faaf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wz\", cfg, False, output_dir=\"./output/\")#output_dir可选的输出目录，用于转储数据集上预测的所有结果。转储包含两个文件 \"instance_predictions.pth\" “ coco_instances_results.json”\n",
        "val_loader = build_detection_test_loader(cfg, \"wz\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/18 02:48:11 d2.data.datasets.coco]: \u001b[0mLoaded 680 images in COCO format from ./drive/My Drive/pic32/train.json\n",
            "\u001b[32m[05/18 02:48:12 d2.data.common]: \u001b[0mSerializing 680 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[05/18 02:48:12 d2.data.common]: \u001b[0mSerialized dataset takes 1.66 MiB\n",
            "\u001b[32m[05/18 02:48:12 d2.evaluation.evaluator]: \u001b[0mStart inference on 680 images\n",
            "\u001b[32m[05/18 02:48:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/680. 0.0643 s / img. ETA=0:00:44\n",
            "\u001b[32m[05/18 02:48:18 d2.evaluation.evaluator]: \u001b[0mInference done 83/680. 0.0666 s / img. ETA=0:00:41\n",
            "\u001b[32m[05/18 02:48:23 d2.evaluation.evaluator]: \u001b[0mInference done 155/680. 0.0664 s / img. ETA=0:00:36\n",
            "\u001b[32m[05/18 02:48:28 d2.evaluation.evaluator]: \u001b[0mInference done 225/680. 0.0668 s / img. ETA=0:00:32\n",
            "\u001b[32m[05/18 02:48:33 d2.evaluation.evaluator]: \u001b[0mInference done 297/680. 0.0666 s / img. ETA=0:00:26\n",
            "\u001b[32m[05/18 02:48:38 d2.evaluation.evaluator]: \u001b[0mInference done 370/680. 0.0665 s / img. ETA=0:00:21\n",
            "\u001b[32m[05/18 02:48:43 d2.evaluation.evaluator]: \u001b[0mInference done 441/680. 0.0665 s / img. ETA=0:00:16\n",
            "\u001b[32m[05/18 02:48:48 d2.evaluation.evaluator]: \u001b[0mInference done 511/680. 0.0667 s / img. ETA=0:00:11\n",
            "\u001b[32m[05/18 02:48:53 d2.evaluation.evaluator]: \u001b[0mInference done 581/680. 0.0668 s / img. ETA=0:00:06\n",
            "\u001b[32m[05/18 02:48:58 d2.evaluation.evaluator]: \u001b[0mInference done 651/680. 0.0669 s / img. ETA=0:00:02\n",
            "\u001b[32m[05/18 02:49:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:47.959891 (0.071052 s / img per device, on 1 devices)\n",
            "\u001b[32m[05/18 02:49:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:45 (0.066997 s / img per device, on 1 devices)\n",
            "\u001b[32m[05/18 02:49:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[05/18 02:49:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
            "\u001b[32m[05/18 02:49:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions ...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.72s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.43s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.903\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.967\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.850\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.893\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.925\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.883\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.950\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.950\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.916\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.942\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.937\n",
            "\u001b[32m[05/18 02:49:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 90.342 | 96.929 | 96.733 | 85.034 | 89.329 | 92.499 |\n",
            "\u001b[32m[05/18 02:49:02 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category      | AP      | category         | AP     | category      | AP     |\n",
            "|:--------------|:--------|:-----------------|:-------|:--------------|:-------|\n",
            "| piezhe        | 89.825  | heng             | 82.671 | hengzhewangou | 93.106 |\n",
            "| pie           | 89.131  | na               | 93.359 | shuwangou     | 90.422 |\n",
            "| henggou       | 87.077  | shugou           | 91.992 | shuzhezhe     | 95.547 |\n",
            "| shu           | 83.862  | hengzhe          | 86.549 | hengzhegou    | 90.729 |\n",
            "| hengzhezhezhe | 100.000 | hengzhezhezhegou | 91.000 | hengpie       | 88.859 |\n",
            "| hengzhezhe    | 95.567  | shuzhezhegou     | 90.885 | dian          | 82.538 |\n",
            "| wangou        | 88.211  | ti               | 91.231 | shuti         | 88.902 |\n",
            "| shuzhe        | 85.839  | wogou            | 97.650 | xiegou        | 95.805 |\n",
            "| hengzhezhepie | 94.540  | hengzhewan       | 93.281 | piedian       | 94.109 |\n",
            "| shuzhepie     | 95.050  | hengxiegou       | 98.570 | hengzheti     | 55.122 |\n",
            "| shuwan        | 95.092  | hengpiewangou    | 94.413 |               |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=1.86s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.43s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.863\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.968\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.954\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.510\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.831\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.865\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.848\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.914\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.914\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.912\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.931\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.865\n",
            "\u001b[32m[05/18 02:49:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 86.267 | 96.779 | 95.436 | 51.001 | 83.144 | 86.496 |\n",
            "\u001b[32m[05/18 02:49:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
            "| category      | AP      | category         | AP     | category      | AP     |\n",
            "|:--------------|:--------|:-----------------|:-------|:--------------|:-------|\n",
            "| piezhe        | 84.025  | heng             | 84.470 | hengzhewangou | 80.120 |\n",
            "| pie           | 86.890  | na               | 89.231 | shuwangou     | 87.798 |\n",
            "| henggou       | 85.100  | shugou           | 89.177 | shuzhezhe     | 85.365 |\n",
            "| shu           | 86.808  | hengzhe          | 85.013 | hengzhegou    | 87.160 |\n",
            "| hengzhezhezhe | 100.000 | hengzhezhezhegou | 82.406 | hengpie       | 87.831 |\n",
            "| hengzhezhe    | 91.332  | shuzhezhegou     | 84.327 | dian          | 82.764 |\n",
            "| wangou        | 88.681  | ti               | 89.169 | shuti         | 88.223 |\n",
            "| shuzhe        | 77.339  | wogou            | 96.211 | xiegou        | 87.919 |\n",
            "| hengzhezhepie | 80.405  | hengzhewan       | 91.881 | piedian       | 85.363 |\n",
            "| shuzhepie     | 90.000  | hengxiegou       | 84.152 | hengzheti     | 54.000 |\n",
            "| shuwan        | 94.512  | hengpiewangou    | 92.871 |               |        |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('bbox',\n",
              "              {'AP': 90.3416718745117,\n",
              "               'AP-dian': 82.53758467269509,\n",
              "               'AP-heng': 82.67057791918447,\n",
              "               'AP-henggou': 87.07711784844084,\n",
              "               'AP-hengpie': 88.8585281521117,\n",
              "               'AP-hengpiewangou': 94.41301272984441,\n",
              "               'AP-hengxiegou': 98.56985698569856,\n",
              "               'AP-hengzhe': 86.54937107892759,\n",
              "               'AP-hengzhegou': 90.7289711886034,\n",
              "               'AP-hengzheti': 55.12211221122109,\n",
              "               'AP-hengzhewan': 93.28147100424327,\n",
              "               'AP-hengzhewangou': 93.10643564356435,\n",
              "               'AP-hengzhezhe': 95.56705670567057,\n",
              "               'AP-hengzhezhepie': 94.54020402040206,\n",
              "               'AP-hengzhezhezhe': 100.0,\n",
              "               'AP-hengzhezhezhegou': 90.99952852428099,\n",
              "               'AP-na': 93.35891066427713,\n",
              "               'AP-pie': 89.13138016772601,\n",
              "               'AP-piedian': 94.10891089108911,\n",
              "               'AP-piezhe': 89.82482557424625,\n",
              "               'AP-shu': 83.86214485261235,\n",
              "               'AP-shugou': 91.99193783391324,\n",
              "               'AP-shuti': 88.9017084898145,\n",
              "               'AP-shuwan': 95.09193776520509,\n",
              "               'AP-shuwangou': 90.42197236811835,\n",
              "               'AP-shuzhe': 85.83859257622267,\n",
              "               'AP-shuzhepie': 95.04950495049505,\n",
              "               'AP-shuzhezhe': 95.54738330975955,\n",
              "               'AP-shuzhezhegou': 90.884900990099,\n",
              "               'AP-ti': 91.23101265913498,\n",
              "               'AP-wangou': 88.21090852515734,\n",
              "               'AP-wogou': 97.65016501650165,\n",
              "               'AP-xiegou': 95.80547466511356,\n",
              "               'AP50': 96.92853723916969,\n",
              "               'AP75': 96.73320921162855,\n",
              "               'APl': 92.49935657579827,\n",
              "               'APm': 89.32903033313339,\n",
              "               'APs': 85.03378650523813}),\n",
              "             ('segm',\n",
              "              {'AP': 86.26704115584087,\n",
              "               'AP-dian': 82.76444253106749,\n",
              "               'AP-heng': 84.47022191929378,\n",
              "               'AP-henggou': 85.10044776567784,\n",
              "               'AP-hengpie': 87.83112522912269,\n",
              "               'AP-hengpiewangou': 92.87128712871288,\n",
              "               'AP-hengxiegou': 84.15244381581016,\n",
              "               'AP-hengzhe': 85.012759814455,\n",
              "               'AP-hengzhegou': 87.1597339557305,\n",
              "               'AP-hengzheti': 53.99999999999998,\n",
              "               'AP-hengzhewan': 91.88118811881189,\n",
              "               'AP-hengzhewangou': 80.12022630834512,\n",
              "               'AP-hengzhezhe': 91.33213321332133,\n",
              "               'AP-hengzhezhepie': 80.40476547654767,\n",
              "               'AP-hengzhezhezhe': 100.0,\n",
              "               'AP-hengzhezhezhegou': 82.40629420084866,\n",
              "               'AP-na': 89.23131348832415,\n",
              "               'AP-pie': 86.89019204306558,\n",
              "               'AP-piedian': 85.36303630363035,\n",
              "               'AP-piezhe': 84.02474403323609,\n",
              "               'AP-shu': 86.80754251360325,\n",
              "               'AP-shugou': 89.17689512304973,\n",
              "               'AP-shuti': 88.22261071550504,\n",
              "               'AP-shuwan': 94.51202263083451,\n",
              "               'AP-shuwangou': 87.79828003102769,\n",
              "               'AP-shuzhe': 77.33908413957667,\n",
              "               'AP-shuzhepie': 90.0,\n",
              "               'AP-shuzhezhe': 85.36453645364537,\n",
              "               'AP-shuzhezhegou': 84.32724041634933,\n",
              "               'AP-ti': 89.16925012611631,\n",
              "               'AP-wangou': 88.68141379355325,\n",
              "               'AP-wogou': 96.21074964639321,\n",
              "               'AP-xiegou': 87.91933605125219,\n",
              "               'AP50': 96.77873108909493,\n",
              "               'AP75': 95.43550189203518,\n",
              "               'APl': 86.49591046061128,\n",
              "               'APm': 83.14407413275735,\n",
              "               'APs': 51.00095883641417})])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6lYrCqLLLW",
        "colab_type": "text"
      },
      "source": [
        "## Benchmark inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxRHYcAC_Z0f",
        "colab_type": "code",
        "outputId": "d81bbe88-d55e-4eaa-f2dc-ff89e2fb5d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    outputs = predictor(im)\n",
        "    delta = time.time() - start_time\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1 / mean_delta\n",
        "print(\"Average(sec):{:.2f},fps:{:.2f}\".format(mean_delta, fps))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average(sec):0.08,fps:13.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peA_VTUdmI_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFMOqBbWEh5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}