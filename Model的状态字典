{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "““坚果改造笔画cuda_error”的副本”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiHanWang2Developer/data/blob/master/Model%E7%9A%84%E7%8A%B6%E6%80%81%E5%AD%97%E5%85%B8\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-UPaAdWoVgJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR",
        "colab_type": "text"
      },
      "source": [
        "# [How to train Detectron2 with Custom COCO Datasets](https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/) | DLology\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "This notebook will help you get started with this framwork by training a instance segmentation model with your custom COCO datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bVqmEoGK4jf",
        "colab_type": "text"
      },
      "source": [
        "本文参考https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDC4G20IuIm",
        "colab_type": "code",
        "outputId": "d534db19-47bb-4095-e05f-66597c5e445a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr  4 15:07:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII",
        "colab_type": "text"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U torch torchvision\n",
        "!pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "import torch, torchvision\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeejixTmwEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
        "# clone the repo to access PointRend code. Use the same version as the installed detectron2\n",
        "!git clone --branch v0.1.1 https://github.com/facebookresearch/detectron2 detectron2_repo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# import PointRend project\n",
        "import sys; sys.path.insert(1, \"detectron2_repo/projects/PointRend\")\n",
        "from detectron2_repo.projects.PointRend import point_rend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo",
        "colab_type": "text"
      },
      "source": [
        "# Train on a custom COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_",
        "colab_type": "text"
      },
      "source": [
        "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
        "\n",
        "We use [the fruits nuts segmentation dataset](https://github.com/Tony607/mmdetection_instance_segmentation_demo)\n",
        "which only has 3 classes: data, fig, and hazelnut.\n",
        "We'll train a segmentation model from an existing model pre-trained on the COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that the COCO dataset does not have the \"data\", \"fig\" and \"hazelnut\" categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RhkndJ6JWqO",
        "colab_type": "code",
        "outputId": "24b420ad-06b6-4db5-89db-dc32213422fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qg7zSVOulkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # download, decompress the data\n",
        "# !wget https://github.com/Tony607/detectron2_instance_segmentation_demo/releases/download/V0.1/data.zip\n",
        "# !unzip data.zip > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW",
        "colab_type": "text"
      },
      "source": [
        "Register the fruits_nuts dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnkg1PByUjGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.data.datasets import register_coco_instances\n",
        "# register_coco_instances(\"fruits_nuts\", {}, \"./data/trainval.json\", \"./data/images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWknKqWTWIw9",
        "colab_type": "code",
        "outputId": "2f5a878f-6c4a-46a5-c7b8-1b6dc36f32d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# fruits_nuts_metadata = MetadataCatalog.get(\"fruits_nuts\")\n",
        "# dataset_dicts = DatasetCatalog.get(\"fruits_nuts\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[03/26 02:56:00 d2.data.datasets.coco]: \u001b[0mLoaded 18 images in COCO format from ./data/trainval.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-aG4sj3cV2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "下面 笔画数据集\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Retbdmc07rgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"wz\", {}, \"./drive/My Drive/pic566_28class/images566.json\", \"./drive/My Drive/pic566_28class/images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttCvanr27rPN",
        "colab_type": "code",
        "outputId": "8ee089dc-15c8-4d65-df53-602f36e1a30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wanzheng_metadata = MetadataCatalog.get(\"wz\")\n",
        "wanzhengdataset_dicts = DatasetCatalog.get(\"wz\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 15:12:40 d2.data.datasets.coco]: \u001b[0mLoaded 566 images in COCO format from ./drive/My Drive/pic566_28class/images566.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E",
        "colab_type": "text"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q38FZu0W37T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #坚果数据集\n",
        "# import random\n",
        "\n",
        "# for d in random.sample(dataset_dicts, 1):\n",
        "#    img = cv2.imread(d[\"file_name\"])#!!!!!!!!!!!!!!!!!！！！！！！！！！！！\n",
        "#    visualizer = Visualizer(img[:, :, ::-1], metadata=fruits_nuts_metadata, scale=0.5)\n",
        "#    vis = visualizer.draw_dataset_dict(d)\n",
        "#    cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "#    cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5JPh6Ur8FTD",
        "colab_type": "code",
        "outputId": "bc3600ee-2d85-414a-a6f9-c8825803fea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#笔画数据集\n",
        "import random\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=wanzheng_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(vis.get_image()[:, :, ::-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAkc0lEQVR4nO19eXxV1bXw2vuMd8qc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\n8J8KyW6G/sfD1wToZ/iaAP0M/w/muJpq9Wy1LgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F719F1667B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA",
        "colab_type": "text"
      },
      "source": [
        "Now, let's fine-tune a coco-pretrained R50-FPN Mask R-CNN model on the fruits_nuts dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.engine import DefaultTrainer\n",
        "# from detectron2.config import get_cfg\n",
        "# import os\n",
        "\n",
        "# cfg = get_cfg()\n",
        "# cfg.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "# cfg.DATASETS.TRAIN = (\"fruits_nuts\",)\n",
        "# cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "# cfg.DATALOADER.NUM_WORKERS = 2\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "# cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "# cfg.SOLVER.BASE_LR = 0.02\n",
        "# cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\n",
        "# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # 3 classes (data, fig, hazelnut)\n",
        "\n",
        "# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "# trainer = DefaultTrainer(cfg)\n",
        "# trainer.resume_or_load(resume=False)\n",
        "# trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuB2wY_8kCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Add PointRend-specific config\n",
        "point_rend.add_pointrend_config(cfg)\n",
        "cfg.MODEL.POINT_HEAD.NUM_CLASSES = 28#修改POINT_HEAD.NUM_CLASSES为28 默认值为80\n",
        "\n",
        "# cfg.merge_from_file(\"./drive/My Drive/Colab Notebooks/detectron2_repo/configs/COCO-InstanceSegmentation/Base-PointRend-RCNN-FPN.yaml\")\n",
        "cfg.merge_from_file(\"./drive/My Drive/Colab Notebooks/detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"wz\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_3c3198.pkl\"\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.02\n",
        "cfg.SOLVER.MAX_ITER = 500    # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE =256   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 28  # 28 classes (heng,shu....)\n",
        "# assert cfg.MODEL.ROI_HEADS.NUM_CLASSES == cfg.MODEL.POINT_HEAD.NUM_CLASSES\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVtTbR_A-WBq",
        "colab_type": "code",
        "outputId": "5d001f8c-bae1-460d-d00f-581308c865f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#正式训练\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "       "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 15:13:18 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): PointRendROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=29, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=112, bias=True)\n",
            "    )\n",
            "    (mask_coarse_head): CoarseMaskHead(\n",
            "      (reduce_spatial_dim_conv): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (coarse_mask_fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (coarse_mask_fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (prediction): Linear(in_features=1024, out_features=1372, bias=True)\n",
            "    )\n",
            "    (mask_point_head): StandardPointHead(\n",
            "      (fc1): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc2): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc3): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (predictor): Conv1d(284, 28, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/04 15:13:18 d2.data.datasets.coco]: \u001b[0mLoaded 566 images in COCO format from ./drive/My Drive/pic566_28class/images566.json\n",
            "\u001b[32m[04/04 15:13:18 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 566 images left.\n",
            "\u001b[32m[04/04 15:13:18 d2.data.build]: \u001b[0mDistribution of instances among all 28 categories:\n",
            "\u001b[36m|   category    | #instances   |  category  | #instances   |   category    | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:-------------:|:-------------|\n",
            "|    piezhe     | 55           |    heng    | 1010         | hengzhewangou | 21           |\n",
            "|      pie      | 822          |     na     | 193          |   shuwangou   | 54           |\n",
            "|    henggou    | 45           |   shugou   | 105          |  hengzhegou   | 153          |\n",
            "| hengzhezhez.. | 5            |  hengpie   | 58           |      shu      | 577          |\n",
            "| shuzhezhegou  | 12           |    dian    | 327          |    wangou     | 20           |\n",
            "|      ti       | 123          |   shuti    | 17           |    shuzhe     | 21           |\n",
            "|     wogou     | 8            |  hengzhe   | 165          |    xiegou     | 7            |\n",
            "| hengzhezhepie | 20           | hengzhewan | 6            |    piedian    | 8            |\n",
            "|   shuzhepie   | 2            | hengxiegou | 4            |   hengzheti   | 2            |\n",
            "|    shuwan     | 4            |            |              |               |              |\n",
            "|     total     | 3844         |            |              |               |              |\u001b[0m\n",
            "\u001b[32m[04/04 15:13:18 d2.data.common]: \u001b[0mSerializing 566 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/04 15:13:18 d2.data.common]: \u001b[0mSerialized dataset takes 1.53 MiB\n",
            "\u001b[32m[04/04 15:13:18 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[04/04 15:13:18 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "model_final_3c3198.pkl: 241MB [00:21, 11.4MB/s]                           \n",
            "'roi_heads.box_predictor.cls_score.weight' has shape (81, 1024) in the checkpoint but (29, 1024) in the model! Skipped.\n",
            "'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (29,) in the model! Skipped.\n",
            "'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 1024) in the checkpoint but (112, 1024) in the model! Skipped.\n",
            "'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (112,) in the model! Skipped.\n",
            "'roi_heads.mask_coarse_head.prediction.weight' has shape (3920, 1024) in the checkpoint but (1372, 1024) in the model! Skipped.\n",
            "'roi_heads.mask_coarse_head.prediction.bias' has shape (3920,) in the checkpoint but (1372,) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc1.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc2.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.fc3.weight' has shape (256, 336, 1) in the checkpoint but (256, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.predictor.weight' has shape (80, 336, 1) in the checkpoint but (28, 284, 1) in the model! Skipped.\n",
            "'roi_heads.mask_point_head.predictor.bias' has shape (80,) in the checkpoint but (28,) in the model! Skipped.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 15:13:47 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/04 15:13:59 d2.utils.events]: \u001b[0m eta: 0:04:12  iter: 19  total_loss: 4.869  loss_cls: 2.586  loss_box_reg: 0.920  loss_mask: 0.689  loss_mask_point: 0.654  loss_rpn_cls: 0.063  loss_rpn_loc: 0.079  time: 0.5231  data_time: 0.3242  lr: 0.000400  max_mem: 2454M\n",
            "\u001b[32m[04/04 15:14:09 d2.utils.events]: \u001b[0m eta: 0:03:56  iter: 39  total_loss: 3.043  loss_cls: 1.038  loss_box_reg: 0.887  loss_mask: 0.590  loss_mask_point: 0.498  loss_rpn_cls: 0.024  loss_rpn_loc: 0.069  time: 0.5192  data_time: 0.2956  lr: 0.000799  max_mem: 2483M\n",
            "\u001b[32m[04/04 15:14:19 d2.utils.events]: \u001b[0m eta: 0:03:45  iter: 59  total_loss: 2.573  loss_cls: 0.803  loss_box_reg: 0.844  loss_mask: 0.477  loss_mask_point: 0.355  loss_rpn_cls: 0.008  loss_rpn_loc: 0.056  time: 0.5106  data_time: 0.2722  lr: 0.001199  max_mem: 2483M\n",
            "\u001b[32m[04/04 15:14:29 d2.utils.events]: \u001b[0m eta: 0:03:34  iter: 79  total_loss: 2.031  loss_cls: 0.660  loss_box_reg: 0.659  loss_mask: 0.355  loss_mask_point: 0.285  loss_rpn_cls: 0.013  loss_rpn_loc: 0.047  time: 0.5148  data_time: 0.2974  lr: 0.001598  max_mem: 2484M\n",
            "\u001b[32m[04/04 15:14:42 d2.utils.events]: \u001b[0m eta: 0:03:23  iter: 99  total_loss: 1.559  loss_cls: 0.452  loss_box_reg: 0.521  loss_mask: 0.255  loss_mask_point: 0.258  loss_rpn_cls: 0.009  loss_rpn_loc: 0.063  time: 0.5410  data_time: 0.4263  lr: 0.001998  max_mem: 2484M\n",
            "\u001b[32m[04/04 15:14:52 d2.utils.events]: \u001b[0m eta: 0:03:12  iter: 119  total_loss: 1.509  loss_cls: 0.432  loss_box_reg: 0.500  loss_mask: 0.234  loss_mask_point: 0.238  loss_rpn_cls: 0.006  loss_rpn_loc: 0.056  time: 0.5299  data_time: 0.2555  lr: 0.002398  max_mem: 2484M\n",
            "\u001b[32m[04/04 15:15:01 d2.utils.events]: \u001b[0m eta: 0:03:01  iter: 139  total_loss: 1.353  loss_cls: 0.393  loss_box_reg: 0.445  loss_mask: 0.226  loss_mask_point: 0.263  loss_rpn_cls: 0.009  loss_rpn_loc: 0.077  time: 0.5222  data_time: 0.2653  lr: 0.002797  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:11 d2.utils.events]: \u001b[0m eta: 0:02:51  iter: 159  total_loss: 1.413  loss_cls: 0.413  loss_box_reg: 0.451  loss_mask: 0.214  loss_mask_point: 0.252  loss_rpn_cls: 0.012  loss_rpn_loc: 0.058  time: 0.5192  data_time: 0.2822  lr: 0.003197  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:21 d2.utils.events]: \u001b[0m eta: 0:02:40  iter: 179  total_loss: 1.221  loss_cls: 0.334  loss_box_reg: 0.404  loss_mask: 0.202  loss_mask_point: 0.219  loss_rpn_cls: 0.007  loss_rpn_loc: 0.052  time: 0.5131  data_time: 0.2502  lr: 0.003596  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:30 d2.utils.events]: \u001b[0m eta: 0:02:30  iter: 199  total_loss: 1.227  loss_cls: 0.373  loss_box_reg: 0.382  loss_mask: 0.202  loss_mask_point: 0.211  loss_rpn_cls: 0.009  loss_rpn_loc: 0.040  time: 0.5085  data_time: 0.2503  lr: 0.003996  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:39 d2.utils.events]: \u001b[0m eta: 0:02:20  iter: 219  total_loss: 1.255  loss_cls: 0.332  loss_box_reg: 0.428  loss_mask: 0.184  loss_mask_point: 0.206  loss_rpn_cls: 0.008  loss_rpn_loc: 0.050  time: 0.5026  data_time: 0.2202  lr: 0.004396  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:48 d2.utils.events]: \u001b[0m eta: 0:02:09  iter: 239  total_loss: 1.239  loss_cls: 0.341  loss_box_reg: 0.410  loss_mask: 0.201  loss_mask_point: 0.185  loss_rpn_cls: 0.010  loss_rpn_loc: 0.062  time: 0.4981  data_time: 0.2215  lr: 0.004795  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:15:57 d2.utils.events]: \u001b[0m eta: 0:01:59  iter: 259  total_loss: 1.119  loss_cls: 0.295  loss_box_reg: 0.383  loss_mask: 0.177  loss_mask_point: 0.182  loss_rpn_cls: 0.012  loss_rpn_loc: 0.052  time: 0.4955  data_time: 0.2418  lr: 0.005195  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:07 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 279  total_loss: 1.115  loss_cls: 0.285  loss_box_reg: 0.362  loss_mask: 0.156  loss_mask_point: 0.199  loss_rpn_cls: 0.008  loss_rpn_loc: 0.062  time: 0.4937  data_time: 0.2461  lr: 0.005594  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:13 d2.utils.events]: \u001b[0m eta: 0:01:39  iter: 299  total_loss: 1.080  loss_cls: 0.310  loss_box_reg: 0.357  loss_mask: 0.152  loss_mask_point: 0.190  loss_rpn_cls: 0.013  loss_rpn_loc: 0.066  time: 0.4818  data_time: 0.0485  lr: 0.005994  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:19 d2.utils.events]: \u001b[0m eta: 0:01:27  iter: 319  total_loss: 0.966  loss_cls: 0.243  loss_box_reg: 0.354  loss_mask: 0.139  loss_mask_point: 0.172  loss_rpn_cls: 0.007  loss_rpn_loc: 0.057  time: 0.4695  data_time: 0.0075  lr: 0.006394  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:25 d2.utils.events]: \u001b[0m eta: 0:01:16  iter: 339  total_loss: 0.990  loss_cls: 0.279  loss_box_reg: 0.336  loss_mask: 0.149  loss_mask_point: 0.170  loss_rpn_cls: 0.011  loss_rpn_loc: 0.065  time: 0.4587  data_time: 0.0083  lr: 0.006793  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:30 d2.utils.events]: \u001b[0m eta: 0:01:05  iter: 359  total_loss: 0.927  loss_cls: 0.222  loss_box_reg: 0.354  loss_mask: 0.126  loss_mask_point: 0.179  loss_rpn_cls: 0.007  loss_rpn_loc: 0.053  time: 0.4491  data_time: 0.0089  lr: 0.007193  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:36 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 379  total_loss: 0.909  loss_cls: 0.218  loss_box_reg: 0.325  loss_mask: 0.117  loss_mask_point: 0.162  loss_rpn_cls: 0.007  loss_rpn_loc: 0.050  time: 0.4404  data_time: 0.0078  lr: 0.007592  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:42 d2.utils.events]: \u001b[0m eta: 0:00:44  iter: 399  total_loss: 0.964  loss_cls: 0.240  loss_box_reg: 0.318  loss_mask: 0.128  loss_mask_point: 0.160  loss_rpn_cls: 0.011  loss_rpn_loc: 0.055  time: 0.4324  data_time: 0.0085  lr: 0.007992  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:48 d2.utils.events]: \u001b[0m eta: 0:00:34  iter: 419  total_loss: 1.019  loss_cls: 0.261  loss_box_reg: 0.334  loss_mask: 0.128  loss_mask_point: 0.181  loss_rpn_cls: 0.007  loss_rpn_loc: 0.069  time: 0.4258  data_time: 0.0086  lr: 0.008392  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:53 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 439  total_loss: 1.004  loss_cls: 0.249  loss_box_reg: 0.344  loss_mask: 0.131  loss_mask_point: 0.177  loss_rpn_cls: 0.010  loss_rpn_loc: 0.067  time: 0.4197  data_time: 0.0085  lr: 0.008791  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:16:59 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 459  total_loss: 0.869  loss_cls: 0.188  loss_box_reg: 0.321  loss_mask: 0.119  loss_mask_point: 0.161  loss_rpn_cls: 0.007  loss_rpn_loc: 0.056  time: 0.4141  data_time: 0.0096  lr: 0.009191  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:17:05 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 479  total_loss: 0.917  loss_cls: 0.223  loss_box_reg: 0.316  loss_mask: 0.113  loss_mask_point: 0.150  loss_rpn_cls: 0.009  loss_rpn_loc: 0.069  time: 0.4089  data_time: 0.0092  lr: 0.009590  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:17:12 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 1.052  loss_cls: 0.307  loss_box_reg: 0.360  loss_mask: 0.135  loss_mask_point: 0.157  loss_rpn_cls: 0.012  loss_rpn_loc: 0.060  time: 0.4039  data_time: 0.0090  lr: 0.009990  max_mem: 2486M\n",
            "\u001b[32m[04/04 15:17:13 d2.engine.hooks]: \u001b[0mOverall training speed: 497 iterations in 0:03:21 (0.4047 s / it)\n",
            "\u001b[32m[04/04 15:17:13 d2.engine.hooks]: \u001b[0mTotal training time: 0:03:23 (0:00:02 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivOaw1hgwLui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9bf0e97-516d-4c8d-dd03-8ccc1cda7bb2"
      },
      "source": [
        "#获取模型model   def build_model(cls, cfg):Returns: torch.nn.Module:\n",
        "PointRendModel = trainer.build_model(cfg)  \n",
        "# print(PointRendModel) \n",
        "\n",
        "print('------------------------完美的分割线-------------------------------------')\n",
        "\n",
        "# 打印模型的状态字典\n",
        "print(\"PointRendModel's state_dict:\")\n",
        "for param_tensor in PointRendModel.state_dict():\n",
        "    print(param_tensor, \"\\t\", PointRendModel.state_dict()[param_tensor].size())\n",
        "# torch.save(model, 'myModel.pth')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/04 15:26:07 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): PointRendROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=29, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=112, bias=True)\n",
            "    )\n",
            "    (mask_coarse_head): CoarseMaskHead(\n",
            "      (reduce_spatial_dim_conv): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (coarse_mask_fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (coarse_mask_fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (prediction): Linear(in_features=1024, out_features=1372, bias=True)\n",
            "    )\n",
            "    (mask_point_head): StandardPointHead(\n",
            "      (fc1): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc2): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (fc3): Conv1d(284, 256, kernel_size=(1,), stride=(1,))\n",
            "      (predictor): Conv1d(284, 28, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "------------------------完美的分割线-------------------------------------\n",
            "PointRendModel's state_dict:\n",
            "backbone.fpn_lateral2.weight \t torch.Size([256, 256, 1, 1])\n",
            "backbone.fpn_lateral2.bias \t torch.Size([256])\n",
            "backbone.fpn_output2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.fpn_output2.bias \t torch.Size([256])\n",
            "backbone.fpn_lateral3.weight \t torch.Size([256, 512, 1, 1])\n",
            "backbone.fpn_lateral3.bias \t torch.Size([256])\n",
            "backbone.fpn_output3.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.fpn_output3.bias \t torch.Size([256])\n",
            "backbone.fpn_lateral4.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.fpn_lateral4.bias \t torch.Size([256])\n",
            "backbone.fpn_output4.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.fpn_output4.bias \t torch.Size([256])\n",
            "backbone.fpn_lateral5.weight \t torch.Size([256, 2048, 1, 1])\n",
            "backbone.fpn_lateral5.bias \t torch.Size([256])\n",
            "backbone.fpn_output5.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.fpn_output5.bias \t torch.Size([256])\n",
            "backbone.bottom_up.stem.conv1.weight \t torch.Size([64, 3, 7, 7])\n",
            "backbone.bottom_up.stem.conv1.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.stem.conv1.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.stem.conv1.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.stem.conv1.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.shortcut.weight \t torch.Size([256, 64, 1, 1])\n",
            "backbone.bottom_up.res2.0.shortcut.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.shortcut.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.shortcut.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.shortcut.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
            "backbone.bottom_up.res2.0.conv1.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv1.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv1.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv1.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "backbone.bottom_up.res2.0.conv2.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv2.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv2.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv2.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
            "backbone.bottom_up.res2.0.conv3.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.conv3.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.conv3.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res2.0.conv3.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res2.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
            "backbone.bottom_up.res2.1.conv1.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv1.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv1.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv1.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "backbone.bottom_up.res2.1.conv2.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv2.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv2.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv2.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
            "backbone.bottom_up.res2.1.conv3.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res2.1.conv3.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res2.1.conv3.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res2.1.conv3.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res2.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
            "backbone.bottom_up.res2.2.conv1.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv1.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv1.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv1.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "backbone.bottom_up.res2.2.conv2.norm.weight \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv2.norm.bias \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv2.norm.running_mean \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv2.norm.running_var \t torch.Size([64])\n",
            "backbone.bottom_up.res2.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
            "backbone.bottom_up.res2.2.conv3.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res2.2.conv3.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res2.2.conv3.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res2.2.conv3.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res3.0.shortcut.weight \t torch.Size([512, 256, 1, 1])\n",
            "backbone.bottom_up.res3.0.shortcut.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.shortcut.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.shortcut.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.shortcut.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
            "backbone.bottom_up.res3.0.conv1.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv1.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv1.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv1.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
            "backbone.bottom_up.res3.0.conv2.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv2.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv2.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv2.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
            "backbone.bottom_up.res3.0.conv3.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.conv3.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.conv3.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res3.0.conv3.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res3.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
            "backbone.bottom_up.res3.1.conv1.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv1.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv1.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv1.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
            "backbone.bottom_up.res3.1.conv2.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv2.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv2.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv2.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
            "backbone.bottom_up.res3.1.conv3.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res3.1.conv3.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res3.1.conv3.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res3.1.conv3.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res3.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
            "backbone.bottom_up.res3.2.conv1.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv1.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv1.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv1.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
            "backbone.bottom_up.res3.2.conv2.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv2.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv2.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv2.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
            "backbone.bottom_up.res3.2.conv3.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res3.2.conv3.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res3.2.conv3.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res3.2.conv3.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res3.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
            "backbone.bottom_up.res3.3.conv1.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv1.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv1.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv1.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
            "backbone.bottom_up.res3.3.conv2.norm.weight \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv2.norm.bias \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv2.norm.running_mean \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv2.norm.running_var \t torch.Size([128])\n",
            "backbone.bottom_up.res3.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
            "backbone.bottom_up.res3.3.conv3.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res3.3.conv3.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res3.3.conv3.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res3.3.conv3.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res4.0.shortcut.weight \t torch.Size([1024, 512, 1, 1])\n",
            "backbone.bottom_up.res4.0.shortcut.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.shortcut.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.shortcut.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.shortcut.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
            "backbone.bottom_up.res4.0.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.0.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.0.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.0.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.bottom_up.res4.1.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.1.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.1.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.1.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.1.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.1.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.bottom_up.res4.2.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.2.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.2.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.2.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.2.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.2.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.bottom_up.res4.3.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.3.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.3.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.3.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.3.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.3.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.bottom_up.res4.4.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.4.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.4.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.4.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.4.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.4.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
            "backbone.bottom_up.res4.5.conv1.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv1.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv1.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv1.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
            "backbone.bottom_up.res4.5.conv2.norm.weight \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv2.norm.bias \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv2.norm.running_mean \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv2.norm.running_var \t torch.Size([256])\n",
            "backbone.bottom_up.res4.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
            "backbone.bottom_up.res4.5.conv3.norm.weight \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.5.conv3.norm.bias \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.5.conv3.norm.running_mean \t torch.Size([1024])\n",
            "backbone.bottom_up.res4.5.conv3.norm.running_var \t torch.Size([1024])\n",
            "backbone.bottom_up.res5.0.shortcut.weight \t torch.Size([2048, 1024, 1, 1])\n",
            "backbone.bottom_up.res5.0.shortcut.norm.weight \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.shortcut.norm.bias \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.shortcut.norm.running_mean \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.shortcut.norm.running_var \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
            "backbone.bottom_up.res5.0.conv1.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv1.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv1.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv1.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
            "backbone.bottom_up.res5.0.conv2.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv2.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv2.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv2.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
            "backbone.bottom_up.res5.0.conv3.norm.weight \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.conv3.norm.bias \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.conv3.norm.running_mean \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.0.conv3.norm.running_var \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
            "backbone.bottom_up.res5.1.conv1.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv1.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv1.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv1.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
            "backbone.bottom_up.res5.1.conv2.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv2.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv2.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv2.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
            "backbone.bottom_up.res5.1.conv3.norm.weight \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.1.conv3.norm.bias \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.1.conv3.norm.running_mean \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.1.conv3.norm.running_var \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
            "backbone.bottom_up.res5.2.conv1.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv1.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv1.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv1.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
            "backbone.bottom_up.res5.2.conv2.norm.weight \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv2.norm.bias \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv2.norm.running_mean \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv2.norm.running_var \t torch.Size([512])\n",
            "backbone.bottom_up.res5.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
            "backbone.bottom_up.res5.2.conv3.norm.weight \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.2.conv3.norm.bias \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.2.conv3.norm.running_mean \t torch.Size([2048])\n",
            "backbone.bottom_up.res5.2.conv3.norm.running_var \t torch.Size([2048])\n",
            "proposal_generator.anchor_generator.cell_anchors.0 \t torch.Size([3, 4])\n",
            "proposal_generator.anchor_generator.cell_anchors.1 \t torch.Size([3, 4])\n",
            "proposal_generator.anchor_generator.cell_anchors.2 \t torch.Size([3, 4])\n",
            "proposal_generator.anchor_generator.cell_anchors.3 \t torch.Size([3, 4])\n",
            "proposal_generator.anchor_generator.cell_anchors.4 \t torch.Size([3, 4])\n",
            "proposal_generator.rpn_head.conv.weight \t torch.Size([256, 256, 3, 3])\n",
            "proposal_generator.rpn_head.conv.bias \t torch.Size([256])\n",
            "proposal_generator.rpn_head.objectness_logits.weight \t torch.Size([3, 256, 1, 1])\n",
            "proposal_generator.rpn_head.objectness_logits.bias \t torch.Size([3])\n",
            "proposal_generator.rpn_head.anchor_deltas.weight \t torch.Size([12, 256, 1, 1])\n",
            "proposal_generator.rpn_head.anchor_deltas.bias \t torch.Size([12])\n",
            "roi_heads.box_head.fc1.weight \t torch.Size([1024, 12544])\n",
            "roi_heads.box_head.fc1.bias \t torch.Size([1024])\n",
            "roi_heads.box_head.fc2.weight \t torch.Size([1024, 1024])\n",
            "roi_heads.box_head.fc2.bias \t torch.Size([1024])\n",
            "roi_heads.box_predictor.cls_score.weight \t torch.Size([29, 1024])\n",
            "roi_heads.box_predictor.cls_score.bias \t torch.Size([29])\n",
            "roi_heads.box_predictor.bbox_pred.weight \t torch.Size([112, 1024])\n",
            "roi_heads.box_predictor.bbox_pred.bias \t torch.Size([112])\n",
            "roi_heads.mask_coarse_head.reduce_spatial_dim_conv.weight \t torch.Size([256, 256, 2, 2])\n",
            "roi_heads.mask_coarse_head.reduce_spatial_dim_conv.bias \t torch.Size([256])\n",
            "roi_heads.mask_coarse_head.coarse_mask_fc1.weight \t torch.Size([1024, 12544])\n",
            "roi_heads.mask_coarse_head.coarse_mask_fc1.bias \t torch.Size([1024])\n",
            "roi_heads.mask_coarse_head.coarse_mask_fc2.weight \t torch.Size([1024, 1024])\n",
            "roi_heads.mask_coarse_head.coarse_mask_fc2.bias \t torch.Size([1024])\n",
            "roi_heads.mask_coarse_head.prediction.weight \t torch.Size([1372, 1024])\n",
            "roi_heads.mask_coarse_head.prediction.bias \t torch.Size([1372])\n",
            "roi_heads.mask_point_head.fc1.weight \t torch.Size([256, 284, 1])\n",
            "roi_heads.mask_point_head.fc1.bias \t torch.Size([256])\n",
            "roi_heads.mask_point_head.fc2.weight \t torch.Size([256, 284, 1])\n",
            "roi_heads.mask_point_head.fc2.bias \t torch.Size([256])\n",
            "roi_heads.mask_point_head.fc3.weight \t torch.Size([256, 284, 1])\n",
            "roi_heads.mask_point_head.fc3.bias \t torch.Size([256])\n",
            "roi_heads.mask_point_head.predictor.weight \t torch.Size([28, 284, 1])\n",
            "roi_heads.mask_point_head.predictor.bias \t torch.Size([28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF",
        "colab_type": "text"
      },
      "source": [
        "Now, we perform inference with the trained model on the fruits_nuts dataset. First, let's create a predictor using the model we just trained:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6RCjvB9vU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "cfg.DATASETS.TEST = (\"wz\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO",
        "colab_type": "text"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM",
        "colab_type": "code",
        "outputId": "d0b9eb5b-954d-4a2b-dfb0-95e600bea763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "for d in random.sample(wanzhengdataset_dicts, 1):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=wanzheng_metadata, \n",
        "                   scale=0.8, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels用于实例化可视化的不同颜色模式  IMAGE_BW：与IMAGE相同，但将所有不带遮罩的区域转换为灰度。仅适用于按实例绘制蒙版预测\n",
        "    )\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "\n",
        "\n",
        "    #如何输出单独的mask\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMwAAADMCAIAAACwQNulAAA58ElEQVR4nO19eXxU1dn/c86528xk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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=204x204 at 0x7F6C712FFD30>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLH2qPSMgOSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "#我们解释一下detectron2中内置模型使用的输入/输出格式https://detectron2.readthedocs.io/tutorials/models.html\n",
        "outputs[\"instances\"].pred_classes\n",
        "outputs[\"instances\"].pred_boxes\n",
        "print(outputs[\"instances\"].pred_masks)\n",
        "#如何可视化mask?\n",
        "#cv2_imshow(outputs[\"instances\"].pred_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bo0cypwllj",
        "colab_type": "code",
        "outputId": "306f13f0-fac2-4897-910d-07b08b30e5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "wanzheng_metadata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metadata(evaluator_type='coco', image_root='./drive/My Drive/pic566_28class/images', json_file='./drive/My Drive/pic566_28class/images566.json', name='wz', thing_classes=['piezhe', 'heng', 'hengzhewangou', 'pie', 'na', 'shuwangou', 'henggou', 'shugou', 'hengzhegou', 'hengzhezhezhegou', 'hengpie', 'shu', 'shuzhezhegou', 'dian', 'wangou', 'ti', 'shuti', 'shuzhe', 'wogou', 'hengzhe', 'xiegou', 'hengzhezhepie', 'hengzhewan', 'piedian', 'shuzhepie', 'hengxiegou', 'hengzheti', 'shuwan'], thing_dataset_id_to_contiguous_id={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvn2tueICLiE",
        "colab_type": "text"
      },
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API. This gives an AP of ~70%. Not bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Y_TQ6YCOWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"wz\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"wz\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6lYrCqLLLW",
        "colab_type": "text"
      },
      "source": [
        "## Benchmark inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxRHYcAC_Z0f",
        "colab_type": "code",
        "outputId": "1b93541d-ed1d-4fb5-ff55-1427970e38df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import time\n",
        "times = []\n",
        "for i in range(20):\n",
        "    start_time = time.time()\n",
        "    outputs = predictor(im)\n",
        "    delta = time.time() - start_time\n",
        "    times.append(delta)\n",
        "mean_delta = np.array(times).mean()\n",
        "fps = 1 / mean_delta\n",
        "print(\"Average(sec):{:.2f},fps:{:.2f}\".format(mean_delta, fps))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average(sec):0.07,fps:13.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFMOqBbWEh5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}